<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Godwill">
<meta name="dcterms.date" content="2026-02-16">
<meta name="description" content="You minimise loss functions every day. MSE, cross-entropy, MAE — they’re the core of model training. But each one is an expected value in disguise. Understanding that connection changes how you think about every model you build.">

<title>Expected Value, Variance, and Why Your Loss Function Is a Statistic – Stats Beneath</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-9e15f8fb9e1f2f288bfbe28f6ca11029.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-9e15f8fb9e1f2f288bfbe28f6ca11029.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-9e15f8fb9e1f2f288bfbe28f6ca11029.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-2b5c9bcf425a053c45e5000656919bfa.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-0c8d6c9924f93c509488b80bc4ab001c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-2b5c9bcf425a053c45e5000656919bfa.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<!-- Google tag (gtag.js) -->

<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PZV1Y6DP51"></script>

<script>

  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());



  gtag('config', 'G-PZV1Y6DP51');

</script>


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="../../homepage.css">
<meta property="og:title" content="Expected Value, Variance, and Why Your Loss Function Is a Statistic – Stats Beneath">
<meta property="og:description" content="You minimise loss functions every day. MSE, cross-entropy, MAE — they’re the core of model training. But each one is an expected value in disguise. Understanding that connection changes how you think about every model you build.">
<meta property="og:image" content="https://statsbeneath.com/posts/03-expected-value-loss-functions/thumbnail.png">
<meta property="og:site_name" content="Stats Beneath">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../images/logo.png" alt="" class="navbar-logo light-content">
    <img src="../../images/logo.png" alt="" class="navbar-logo dark-content">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Stats Beneath</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../archive.html"> 
<span class="menu-text">Articles</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../#newsletter"> 
<span class="menu-text">Subscribe</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/godwillA33peo"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/godwill-zulu/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Expected Value, Variance, and Why Your Loss Function Is a Statistic</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#you-already-use-expected-values.-you-just-dont-call-them-that." id="toc-you-already-use-expected-values.-you-just-dont-call-them-that." class="nav-link active" data-scroll-target="#you-already-use-expected-values.-you-just-dont-call-them-that.">You already use expected values. You just don’t call them that.</a></li>
  <li><a href="#expected-value-the-long-run-average" id="toc-expected-value-the-long-run-average" class="nav-link" data-scroll-target="#expected-value-the-long-run-average">Expected value: the long-run average</a>
  <ul class="collapse">
  <li><a href="#a-concrete-example" id="toc-a-concrete-example" class="nav-link" data-scroll-target="#a-concrete-example">A concrete example</a></li>
  <li><a href="#expected-value-of-a-function" id="toc-expected-value-of-a-function" class="nav-link" data-scroll-target="#expected-value-of-a-function">Expected value of a function</a></li>
  </ul></li>
  <li><a href="#variance-how-spread-out-is-the-distribution" id="toc-variance-how-spread-out-is-the-distribution" class="nav-link" data-scroll-target="#variance-how-spread-out-is-the-distribution">Variance: how spread out is the distribution?</a>
  <ul class="collapse">
  <li><a href="#why-variance-matters-for-ml" id="toc-why-variance-matters-for-ml" class="nav-link" data-scroll-target="#why-variance-matters-for-ml">Why variance matters for ML</a></li>
  </ul></li>
  <li><a href="#covariance-when-random-variables-move-together" id="toc-covariance-when-random-variables-move-together" class="nav-link" data-scroll-target="#covariance-when-random-variables-move-together">Covariance: when random variables move together</a></li>
  <li><a href="#now-the-connection-that-changes-everything" id="toc-now-the-connection-that-changes-everything" class="nav-link" data-scroll-target="#now-the-connection-that-changes-everything">Now: the connection that changes everything</a></li>
  <li><a href="#loss-functions-are-expected-values-the-specifics" id="toc-loss-functions-are-expected-values-the-specifics" class="nav-link" data-scroll-target="#loss-functions-are-expected-values-the-specifics">Loss functions are expected values: the specifics</a>
  <ul class="collapse">
  <li><a href="#mean-squared-error" id="toc-mean-squared-error" class="nav-link" data-scroll-target="#mean-squared-error">Mean Squared Error</a></li>
  <li><a href="#mean-absolute-error" id="toc-mean-absolute-error" class="nav-link" data-scroll-target="#mean-absolute-error">Mean Absolute Error</a></li>
  <li><a href="#cross-entropy-log-loss" id="toc-cross-entropy-log-loss" class="nav-link" data-scroll-target="#cross-entropy-log-loss">Cross-Entropy (Log Loss)</a></li>
  <li><a href="#the-unifying-view" id="toc-the-unifying-view" class="nav-link" data-scroll-target="#the-unifying-view">The unifying view</a></li>
  </ul></li>
  <li><a href="#properties-of-expected-value-that-make-ml-work" id="toc-properties-of-expected-value-that-make-ml-work" class="nav-link" data-scroll-target="#properties-of-expected-value-that-make-ml-work">Properties of expected value that make ML work</a></li>
  <li><a href="#the-sample-mean-is-an-estimator-and-it-has-variance" id="toc-the-sample-mean-is-an-estimator-and-it-has-variance" class="nav-link" data-scroll-target="#the-sample-mean-is-an-estimator-and-it-has-variance">The sample mean is an estimator — and it has variance</a></li>
  <li><a href="#what-this-means-in-practice" id="toc-what-this-means-in-practice" class="nav-link" data-scroll-target="#what-this-means-in-practice">What this means in practice</a></li>
  <li><a href="#the-mental-model-to-take-away" id="toc-the-mental-model-to-take-away" class="nav-link" data-scroll-target="#the-mental-model-to-take-away">The mental model to take away</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Expected Value, Variance, and Why Your Loss Function Is a Statistic</h1>
  <div class="quarto-categories">
    <div class="quarto-category">foundations</div>
    <div class="quarto-category">probability</div>
  </div>
  </div>

<div>
  <div class="description">
    You minimise loss functions every day. MSE, cross-entropy, MAE — they’re the core of model training. But each one is an expected value in disguise. Understanding that connection changes how you think about every model you build.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Godwill </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 16, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>What you’ll learn in 10 minutes
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Expected value</strong> is the bridge between probability distributions and the numbers you compute from data — and you’re already using it every time you evaluate a loss function.</li>
<li><strong>Variance</strong> measures spread around the mean, and it quietly controls everything from feature scaling to overfitting to how noisy your test accuracy is.</li>
<li><strong>Every loss function is an expected value in disguise</strong>: MSE estimates <span class="math inline">\(E[(Y-\hat{Y})^2]\)</span> under Gaussian errors, cross-entropy estimates <span class="math inline">\(-E[Y\log\hat{P} + \ldots]\)</span> under Bernoulli targets, MAE targets the median under Laplace errors.</li>
<li><strong>When you pick a loss, you pick a distribution.</strong> When you train, you minimise an expected value. When you evaluate, you estimate one from a finite sample.</li>
</ul>
</div>
</div>
<section id="you-already-use-expected-values.-you-just-dont-call-them-that." class="level2">
<h2 class="anchored" data-anchor-id="you-already-use-expected-values.-you-just-dont-call-them-that.">You already use expected values. You just don’t call them that.</h2>
<p>Every time you compute an average, you’re computing an expected value. Every time you evaluate a loss function, you’re computing an expected value. Every time you report “my model’s accuracy is 91%,” that number is an expected value.</p>
<p>The concept hides behind so many familiar operations that most ML engineers never pause to examine it directly. That’s a problem, because expected value isn’t just a formula — it’s the <em>bridge</em> between probability distributions (which describe the world) and the numbers you compute from data (which summarise it).</p>
<p>In the <a href="../..\posts/02-random-variables-distributions/index.html">previous article</a>, we established that your data consists of realisations of random variables, and that your model learns a conditional distribution. Now the question is: once you have a distribution, what do you <em>do</em> with it? How do you extract a single useful number from an entire probability distribution?</p>
<p>The answer is expected value. And the specific expected values you’re already computing — without knowing it — are your loss functions.</p>
</section>
<section id="expected-value-the-long-run-average" class="level2">
<h2 class="anchored" data-anchor-id="expected-value-the-long-run-average">Expected value: the long-run average</h2>
<p>Let <span class="math inline">\(X\)</span> be a random variable with some distribution. The <strong>expected value</strong> of <span class="math inline">\(X\)</span>, written <span class="math inline">\(E[X]\)</span>, is the average value you’d get if you could draw from that distribution infinitely many times.</p>
<p>For a <strong>discrete</strong> random variable with values <span class="math inline">\(x_1, x_2, \ldots\)</span> and probabilities <span class="math inline">\(p_1, p_2, \ldots\)</span>:</p>
<p><span class="math display">\[E[X] = \sum_i x_i \, p_i\]</span></p>
<p>For a <strong>continuous</strong> random variable with density <span class="math inline">\(f(x)\)</span>:</p>
<p><span class="math display">\[E[X] = \int_{-\infty}^{\infty} x \, f(x) \, dx\]</span></p>
<p>Both say the same thing: weight each possible value by how likely it is, and add them up.</p>
<section id="a-concrete-example" class="level3">
<h3 class="anchored" data-anchor-id="a-concrete-example">A concrete example</h3>
<p>Roll a fair die. The random variable <span class="math inline">\(X\)</span> is the number that comes up. The expected value is:</p>
<p><span class="math display">\[E[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{6} + 3 \cdot \frac{1}{6} + 4 \cdot \frac{1}{6} + 5 \cdot \frac{1}{6} + 6 \cdot \frac{1}{6} = 3.5\]</span></p>
<p>You’ll never roll 3.5. That’s fine. The expected value isn’t a value you expect to <em>see</em> — it’s the centre of gravity of the distribution. It’s the number your sample mean converges to as you roll more and more times.</p>
</section>
<section id="expected-value-of-a-function" class="level3">
<h3 class="anchored" data-anchor-id="expected-value-of-a-function">Expected value of a function</h3>
<p>Here’s where things get powerful. You can take the expected value of <em>any function</em> of a random variable, not just <span class="math inline">\(X\)</span> itself.</p>
<p>If <span class="math inline">\(g(X)\)</span> is some function of <span class="math inline">\(X\)</span>, then:</p>
<p><span class="math display">\[E[g(X)] = \sum_i g(x_i) \, p_i \quad \text{(discrete)}\]</span></p>
<p><span class="math display">\[E[g(X)] = \int g(x) \, f(x) \, dx \quad \text{(continuous)}\]</span></p>
<p>This is called the <strong>Law of the Unconscious Statistician</strong> (yes, really: the name is a gentle jab at people who use it without realising what they’re doing).</p>
<p>Why does this matter? Because loss functions are functions of random variables. And when you average a loss function over your data, you’re estimating the expected value of that function. Let’s make this precise.</p>
</section>
</section>
<section id="variance-how-spread-out-is-the-distribution" class="level2">
<h2 class="anchored" data-anchor-id="variance-how-spread-out-is-the-distribution">Variance: how spread out is the distribution?</h2>
<p>Before we get to loss functions, we need one more concept. <strong>Variance</strong> measures how spread out a random variable is around its expected value:</p>
<p><span class="math display">\[\text{Var}(X) = E\left[(X - E[X])^2\right]\]</span></p>
<p>Read it carefully. Variance is itself an expected value — the expected value of the squared deviation from the mean. It’s <span class="math inline">\(E[g(X)]\)</span> where <span class="math inline">\(g(X) = (X - \mu)^2\)</span>.</p>
<p>The square root of variance is the <strong>standard deviation</strong>, <span class="math inline">\(\sigma = \sqrt{\text{Var}(X)}\)</span>, which has the same units as <span class="math inline">\(X\)</span> and is easier to interpret.</p>
<p>A useful alternative formula (derived by expanding the square):</p>
<p><span class="math display">\[\text{Var}(X) = E[X^2] - (E[X])^2\]</span></p>
<p>This tells you: the variance is the expected value of the square minus the square of the expected value. Or more memorably: <strong>“the mean of the squares minus the square of the mean.”</strong></p>
<section id="why-variance-matters-for-ml" class="level3">
<h3 class="anchored" data-anchor-id="why-variance-matters-for-ml">Why variance matters for ML</h3>
<p>Variance shows up everywhere:</p>
<p><strong>In your data:</strong> The variance of your features determines how much information they carry. A feature with zero variance is useless: it tells you nothing. Feature scaling (standardisation) divides by standard deviation precisely to put all features on equal footing.</p>
<p><strong>In your model:</strong> The variance of your model’s predictions across different training sets is what we call “model variance” in the bias-variance tradeoff. High variance means your model is sensitive to which specific data points it was trained on, that’s overfitting.</p>
<p><strong>In your estimates:</strong> When you report “accuracy = 0.91”, that number has a variance. It would be different if you’d tested on a different sample. The variance tells you how much to trust it.</p>
</section>
</section>
<section id="covariance-when-random-variables-move-together" class="level2">
<h2 class="anchored" data-anchor-id="covariance-when-random-variables-move-together">Covariance: when random variables move together</h2>
<p>When you have two random variables, you often want to know: do they tend to increase together, or does one go up when the other goes down?</p>
<p><strong>Covariance</strong> measures this:</p>
<p><span class="math display">\[\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\]</span></p>
<p>Positive covariance means they move together. Negative means they move in opposite directions. Zero covariance means there’s no <em>linear</em> relationship (but there could still be a nonlinear one).</p>
<p><strong>Correlation</strong> is covariance normalised to lie between -1 and 1:</p>
<p><span class="math display">\[\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X) \cdot \text{Var}(Y)}}\]</span></p>
<p>Here’s the ML connection that matters: <strong>multicollinearity</strong> happens when your feature variables have high covariance with each other. When features are highly correlated, your regression coefficients become unstable: they have high variance, are hard to interpret, and can flip sign. PCA, which we’ll cover in a future article, works by finding directions in feature space along which the <em>variance</em> is maximised while the <em>covariance</em> between the new directions is zero.</p>
</section>
<section id="now-the-connection-that-changes-everything" class="level2">
<h2 class="anchored" data-anchor-id="now-the-connection-that-changes-everything">Now: the connection that changes everything</h2>
<p>Here’s the payoff. Let’s look at what you’re actually computing when you train a model.</p>
<p>You have training data <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^n\)</span>. You have a model <span class="math inline">\(\hat{y} = f(x; \theta)\)</span> with parameters <span class="math inline">\(\theta\)</span>. You define a loss function <span class="math inline">\(L(y, \hat{y})\)</span> that measures how bad each prediction is. Then you compute:</p>
<p><span class="math display">\[\hat{R}(\theta) = \frac{1}{n} \sum_{i=1}^n L(y_i, f(x_i; \theta))\]</span></p>
<p>This is the <strong>empirical risk</strong>: the average loss over your training data. You minimise it to find the best parameters.</p>
<p>But what are you <em>really</em> doing? You’re computing a <strong>sample average</strong> of the function <span class="math inline">\(L(y, f(x; \theta))\)</span>. And a sample average is an <em>estimate</em> of an expected value:</p>
<p><span class="math display">\[R(\theta) = E[L(Y, f(X; \theta))]\]</span></p>
<p>This is the <strong>true risk</strong>: the expected loss over the entire data-generating distribution. It’s the quantity you actually care about, because it tells you how well your model will perform on <em>new</em> data, not just the training set.</p>
<p><strong>Training is estimating parameters that minimise an expected value. Your loss function is the function inside that expectation.</strong></p>
<p>This is not a metaphor. It’s literally what’s happening when you call <code>model.fit()</code>.</p>
</section>
<section id="loss-functions-are-expected-values-the-specifics" class="level2">
<h2 class="anchored" data-anchor-id="loss-functions-are-expected-values-the-specifics">Loss functions are expected values: the specifics</h2>
<p>Let’s make this concrete for the loss functions you use every day.</p>
<section id="mean-squared-error" class="level3">
<h3 class="anchored" data-anchor-id="mean-squared-error">Mean Squared Error</h3>
<p><span class="math display">\[\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 \quad \approx \quad E\left[(Y - \hat{Y})^2\right]\]</span></p>
<p>The MSE is a sample estimate of the expected squared error. Notice that this looks exactly like the formula for variance, and that’s not a coincidence. If your model predicts the mean perfectly (<span class="math inline">\(\hat{Y} = E[Y|X]\)</span>), then the remaining MSE is the <strong>irreducible variance</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. That’s the noise in your data that no model can eliminate.</p>
<p>Here’s the deeper insight: MSE is the <em>natural</em> loss function when you assume <span class="math inline">\(Y|X \sim \mathcal{N}(\hat{y}, \sigma^2)\)</span>. Minimising MSE is <em>identical</em> to maximising the likelihood under Gaussian errors. We’ll prove this formally in the next article on likelihood, but for now, know that <strong>choosing MSE means assuming your errors are normally distributed.</strong> If they’re not — if they’re skewed, heavy-tailed, or heteroscedastic — MSE may not be the right choice.</p>
</section>
<section id="mean-absolute-error" class="level3">
<h3 class="anchored" data-anchor-id="mean-absolute-error">Mean Absolute Error</h3>
<p><span class="math display">\[\text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i| \quad \approx \quad E\left[|Y - \hat{Y}|\right]\]</span></p>
<p>MAE estimates the expected absolute error. Minimising MAE doesn’t correspond to Gaussian errors: it corresponds to <strong>Laplace (double exponential) errors.</strong> This is why MAE is more robust to outliers: the Laplace distribution has heavier tails than the Gaussian, so it’s less surprised by extreme values.</p>
<p>The best prediction under MAE is the <strong>conditional median</strong> of <span class="math inline">\(Y|X\)</span>, not the mean. This is a key distinction: MSE targets the mean, MAE targets the median. If your data is skewed, these are different numbers and the choice matters.</p>
</section>
<section id="cross-entropy-log-loss" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-log-loss">Cross-Entropy (Log Loss)</h3>
<p>For binary classification with true labels <span class="math inline">\(y_i \in \{0, 1\}\)</span> and predicted probabilities <span class="math inline">\(\hat{p}_i\)</span>:</p>
<p><span class="math display">\[\text{CE} = -\frac{1}{n}\sum_{i=1}^n \left[y_i \log \hat{p}_i + (1-y_i)\log(1 - \hat{p}_i)\right] \quad \approx \quad -E\left[Y\log\hat{P} + (1-Y)\log(1-\hat{P})\right]\]</span></p>
<p>Cross-entropy is the expected negative log-probability assigned to the true outcome. It’s what you get when you do maximum likelihood estimation for a Bernoulli distribution, which is exactly what logistic regression does.</p>
<p>Minimising cross-entropy makes your predicted probabilities <span class="math inline">\(\hat{p}\)</span> as close as possible (in a precise information-theoretic sense) to the true conditional probability <span class="math inline">\(P(Y=1|X)\)</span>.</p>
</section>
<section id="the-unifying-view" class="level3">
<h3 class="anchored" data-anchor-id="the-unifying-view">The unifying view</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">What you compute</th>
<th style="text-align: left;">What it estimates</th>
<th style="text-align: left;">What it assumes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\frac{1}{n}\sum(y_i - \hat{y}_i)^2\)</span></td>
<td style="text-align: left;"><span class="math inline">\(E[(Y-\hat{Y})^2]\)</span></td>
<td style="text-align: left;">Gaussian errors</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\frac{1}{n}\sum\|y_i - \hat{y}_i\|\)</span></td>
<td style="text-align: left;"><span class="math inline">\(E[\|Y-\hat{Y}\|]\)</span></td>
<td style="text-align: left;">Laplace errors</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(-\frac{1}{n}\sum y_i \log\hat{p}_i + \ldots\)</span></td>
<td style="text-align: left;"><span class="math inline">\(-E[Y\log\hat{P} + \ldots]\)</span></td>
<td style="text-align: left;">Bernoulli targets</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\frac{1}{n}\sum(y_i\log\frac{y_i}{\hat{y}_i} - y_i + \hat{y}_i)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(E\left[Y\log\frac{Y}{\hat{Y}} - Y + \hat{Y}\right]\)</span></td>
<td style="text-align: left;">Poisson targets</td>
</tr>
</tbody>
</table>
<p>Every loss function in this table is a sample average estimating an expected value. Every one corresponds to a distributional assumption about your target variable. Every time you pick a loss function, you’re making a statistical assumption whether you intend to or not.</p>
</section>
</section>
<section id="properties-of-expected-value-that-make-ml-work" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-expected-value-that-make-ml-work">Properties of expected value that make ML work</h2>
<p>A few properties of <span class="math inline">\(E[\cdot]\)</span> are quietly holding the entire ML pipeline together:</p>
<p><strong>Linearity:</strong> <span class="math inline">\(E[aX + bY] = aE[X] + bE[Y]\)</span></p>
<p>This is why you can decompose complex losses into simpler components. It’s why regularised loss = data loss + penalty works mathematically. It’s why ensemble methods (averaging multiple models) reduce expected error.</p>
<p><strong>Law of Large Numbers:</strong> As <span class="math inline">\(n \to \infty\)</span>, the sample mean <span class="math inline">\(\bar{X}_n \to E[X]\)</span></p>
<p>This is why training on more data works. Your empirical risk (sample average loss) converges to the true risk (expected loss) as your dataset grows. With enough data, the training loss becomes a reliable estimate of the test loss.</p>
<p><strong>Iterated Expectation (Tower Law):</strong> <span class="math inline">\(E[E[Y|X]] = E[Y]\)</span></p>
<p>This says: if you first compute the expected value of <span class="math inline">\(Y\)</span> within each group defined by <span class="math inline">\(X\)</span>, and then average those, you get the overall expected value of <span class="math inline">\(Y\)</span>. This is the mathematical foundation of the bias-variance decomposition, which we’ll derive in a future article.</p>
</section>
<section id="the-sample-mean-is-an-estimator-and-it-has-variance" class="level2">
<h2 class="anchored" data-anchor-id="the-sample-mean-is-an-estimator-and-it-has-variance">The sample mean is an estimator — and it has variance</h2>
<p>When you compute <span class="math inline">\(\bar{x} = \frac{1}{n}\sum x_i\)</span> from data, you’re estimating <span class="math inline">\(E[X]\)</span> from a finite sample. This estimate is itself a random variable: if you’d drawn different data, you’d get a different <span class="math inline">\(\bar{x}\)</span>.</p>
<p>The variance of the sample mean is:</p>
<p><span class="math display">\[\text{Var}(\bar{X}) = \frac{\text{Var}(X)}{n}\]</span></p>
<p>This one formula explains several things at once:</p>
<p><strong>Why more data helps:</strong> As <span class="math inline">\(n\)</span> increases, <span class="math inline">\(\text{Var}(\bar{X})\)</span> decreases. Your estimate becomes more precise. This is why training on more data is almost always beneficial: you’re reducing the variance of your loss estimates.</p>
<p><strong>Why your test accuracy fluctuates:</strong> If you evaluate on 100 test samples vs.&nbsp;10,000, the variance of your accuracy estimate differs by a factor of 100. Small test sets give noisy estimates.</p>
<p><strong>Why mini-batch SGD works:</strong> Each mini-batch gives you a noisy estimate of the gradient (which is itself an expected value). The variance of that estimate decreases with batch size. Smaller batches = noisier gradients = more variance but faster updates. Larger batches = smoother gradients = less variance but slower updates. The tradeoff is directly governed by <span class="math inline">\(\text{Var}(\bar{X}) = \text{Var}(X)/n\)</span>.</p>
</section>
<section id="what-this-means-in-practice" class="level2">
<h2 class="anchored" data-anchor-id="what-this-means-in-practice">What this means in practice</h2>
<p>Understanding that loss functions are expected values gives you three practical capabilities:</p>
<p><strong>1. You can diagnose loss function mismatch.</strong> If your residuals are heavily skewed, MSE (which assumes Gaussian errors) is pulling your predictions toward a mean that isn’t representative. Switch to MAE (which targets the median) or a quantile loss. If your count data has overdispersion, Poisson deviance will underestimate uncertainty — consider negative binomial loss instead. The distributional assumption behind your loss function must match the actual behaviour of your data.</p>
<p><strong>2. You can understand why your evaluation metrics are noisy.</strong> Your reported accuracy, F1 score, or AUC are all sample averages estimating expected values. They have variance. Reporting them without confidence intervals is like reporting a point estimate without uncertainty: technically a number, practically meaningless. We’ll cover confidence intervals properly in a future article.</p>
<p><strong>3. You can reason about the bias-variance tradeoff.</strong> The expected test error decomposes into bias squared plus variance plus irreducible noise. All three terms are expected values. Understanding this decomposition requires understanding expected value first — which is why we’re building this foundation before tackling bias-variance directly.</p>
</section>
<section id="the-mental-model-to-take-away" class="level2">
<h2 class="anchored" data-anchor-id="the-mental-model-to-take-away">The mental model to take away</h2>
<p><strong>Before:</strong> “I pick MSE for regression and cross-entropy for classification because that’s what the tutorial said.”</p>
<p><strong>After:</strong> “MSE estimates <span class="math inline">\(E[(Y - \hat{Y})^2]\)</span> under a Gaussian assumption. Cross-entropy estimates <span class="math inline">\(-E[Y\log\hat{P} + (1-Y)\log(1-\hat{P})]\)</span> under a Bernoulli assumption. I choose based on the distributional properties of my target variable — and I check whether those assumptions hold.”</p>
<p>That’s the shift. Loss functions aren’t arbitrary choices or convention. They’re expected values derived from specific distributional assumptions. When you pick a loss, you’re picking a distribution. When you train a model, you’re minimising an expected value. When you evaluate, you’re estimating an expected value from a finite sample.</p>
<p>Next week, we’ll close the loop. You now know your data comes from distributions (article 2) and your training process minimises expected values of loss functions derived from those distributions (this article). The missing piece is: <strong>how exactly do you estimate the distribution’s parameters from data?</strong> That’s <strong>likelihood</strong>: the single most important concept in ML that nobody teaches properly.</p>
<hr>
<p><em>This is article 3 of <a href="https://statsbeneath.com">Stats Beneath</a>, a weekly series on the statistical foundations of machine learning. <a href="../..">Subscribe</a> to get each article when it’s published.</em></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/statsbeneath\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2026 Stats Beneath</p>
</div>   
    <div class="nav-footer-center">
<p>Built with <a href="https://quarto.org">Quarto</a> &amp; R</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:godwillzulu51@gmail.com">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>