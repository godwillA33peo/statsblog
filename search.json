[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Everything here is something Iâ€™ve personally used, read, or can vouch for. No affiliate links â€” just resources I genuinely think are worth your time."
  },
  {
    "objectID": "resources.html#books",
    "href": "resources.html#books",
    "title": "Resources",
    "section": "Books",
    "text": "Books\n\n\nğŸ“• The Art of Statistics â€” David Spiegelhalter\nThe best â€œstats for everyoneâ€ book. Spiegelhalter uses real stories to explain statistical thinking without equations. Start here if youâ€™re brand new.\n\n\nğŸ“— Naked Statistics â€” Charles Wheelan\nFunny, accessible, and packed with real-world examples. Great for understanding why statistics matters in everyday life.\n\n\nğŸ“˜ An Introduction to Statistical Learning (ISLR) â€” James, Witten, Hastie, Tibshirani\nThe bridge between statistics and machine learning. Free PDF available at statlearning.com. The R labs are excellent.\n\n\nğŸ“™ Statistical Rethinking â€” Richard McElreath\nIf you want to understand Bayesian statistics properly, this is the gold standard. Challenging but rewarding, with amazing R and Stan code examples."
  },
  {
    "objectID": "resources.html#free-courses",
    "href": "resources.html#free-courses",
    "title": "Resources",
    "section": "Free Courses",
    "text": "Free Courses\n\n\nğŸ“ Khan Academy â€” Statistics & Probability\nVisit â†’\nThe best free foundation. Clear videos, practice exercises, covers everything from basics to inference.\n\n\nğŸ“ StatQuest (YouTube)\nVisit â†’\nJosh Starmer explains statistics and ML with simple visuals and catchy jingles. Surprisingly effective.\n\n\nğŸ“ Harvard CS109 â€” Data Science\nVisit â†’\nFull Harvard course with lectures, labs, and assignments. Bridges stats and practical data science beautifully.\n\n\nğŸ“ Seeing Theory\nVisit â†’\nInteractive visualisations of probability and statistics concepts. Built by Brown University. Beautiful and educational."
  },
  {
    "objectID": "resources.html#tools-we-use",
    "href": "resources.html#tools-we-use",
    "title": "Resources",
    "section": "Tools We Use",
    "text": "Tools We Use\n\n\nğŸ”§ R & Positron\nposit.co/positron\nOur language and IDE of choice. R is unmatched for statistical computing, and Positron is a modern, fast editor built for data science.\n\n\nğŸ”§ Quarto\nquarto.org\nWhat this site is built with. Write documents, blogs, and presentations that mix text, code, and output.\n\n\nğŸ”§ Stan\nmc-stan.org\nThe gold standard for Bayesian modeling. Steep learning curve, but incredibly powerful for serious statistical work.\n\n\nğŸ”§ Shiny\nshiny.posit.co\nBuild interactive web apps from R. Great for creating data explorations and teaching tools."
  },
  {
    "objectID": "resources.html#communities",
    "href": "resources.html#communities",
    "title": "Resources",
    "section": "Communities",
    "text": "Communities\n\n\nğŸ’¬ Cross Validated (Stack Exchange)\nVisit â†’\nThe best Q&A site for statistics. Search before you ask â€” most common questions have excellent answers already.\n\n\nğŸ’¬ R-bloggers\nVisit â†’\nAggregated blog posts from hundreds of R users. Great for discovering tutorials and new packages.\n\n\nğŸ’¬ Stan Forums\nVisit â†’\nIncredibly helpful community for Bayesian modelling questions. The developers themselves answer questions.\n\n\nğŸ’¬ Posit Community\nVisit â†’\nFor R, Shiny, Quarto, and Positron questions. Friendly and welcoming to beginners."
  },
  {
    "objectID": "resources.html#datasets-for-practice",
    "href": "resources.html#datasets-for-practice",
    "title": "Resources",
    "section": "Datasets for Practice",
    "text": "Datasets for Practice\n\n\nğŸ“¦ Tidy Tuesday\nVisit â†’\nWeekly datasets released for the R community. Great for practice with real, messy data.\n\n\nğŸ“¦ Kaggle Datasets\nVisit â†’\nThousands of datasets across every domain. Filter by â€œbeginner friendlyâ€ to start.\n\n\nğŸ“¦ UCI Machine Learning Repository\nVisit â†’\nClassic benchmark datasets used in academic research. The Iris and Wine datasets live here.\n\n\nğŸ“¦ Our World in Data\nVisit â†’\nBeautiful, well-documented global datasets on health, economics, education, and more. Perfect for practice with real-world context.\n\n\n\n\n\n\n\n\n\nTipSuggest a Resource\n\n\n\nKnow something that belongs here? Get in touch â€” weâ€™re always looking for quality recommendations."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html",
    "href": "posts/01-why-stats-matter/index.html",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "",
    "text": "Did you know?\n\nThe term â€œregressionâ€ comes from Francis Galtonâ€™s 1886 study on the heights of parents and children. He noticed that tall parents tended to have slightly shorter children, and short parents had slightly taller ones â€” they â€œregressedâ€ toward the average. The name stuck, even though modern regression has nothing to do with shrinking.\n\nThereâ€™s a pattern I see everywhere. Someone discovers machine learning, gets excited, installs scikit-learn or tidymodels, runs a random forest on a dataset, and gets 94% accuracy. Amazing! Ship it!\nBut then someone asks: â€œWhy did your model predict that?â€ And the answer isâ€¦ silence.\nThis is what happens when we skip the foundation. Machine learning is not magic â€” itâ€™s statistics at scale. And if you donâ€™t understand the statistics beneath the algorithm, youâ€™re building on sand."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#the-rush-to-the-algorithm",
    "href": "posts/01-why-stats-matter/index.html#the-rush-to-the-algorithm",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "",
    "text": "Did you know?\n\nThe term â€œregressionâ€ comes from Francis Galtonâ€™s 1886 study on the heights of parents and children. He noticed that tall parents tended to have slightly shorter children, and short parents had slightly taller ones â€” they â€œregressedâ€ toward the average. The name stuck, even though modern regression has nothing to do with shrinking.\n\nThereâ€™s a pattern I see everywhere. Someone discovers machine learning, gets excited, installs scikit-learn or tidymodels, runs a random forest on a dataset, and gets 94% accuracy. Amazing! Ship it!\nBut then someone asks: â€œWhy did your model predict that?â€ And the answer isâ€¦ silence.\nThis is what happens when we skip the foundation. Machine learning is not magic â€” itâ€™s statistics at scale. And if you donâ€™t understand the statistics beneath the algorithm, youâ€™re building on sand."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#what-beneath-actually-means",
    "href": "posts/01-why-stats-matter/index.html#what-beneath-actually-means",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "What â€œbeneathâ€ actually means",
    "text": "What â€œbeneathâ€ actually means\nEvery machine learning model youâ€™ve ever heard of is built on statistical concepts that have existed for decades:\nLinear regression (1800s) â†’ Neural networks are layers of linear regressions with activation functions stacked on top.\nBayesâ€™ theorem (1763) â†’ The entire field of Bayesian machine learning, spam filters, and medical diagnosis models.\nMatrix multiplication (1850s) â†’ Literally how your data flows through every neural network.\nProbability distributions (1700s) â†’ How models quantify uncertainty, make predictions, and learn from data.\nThe â€œAI revolutionâ€ isnâ€™t new math. Itâ€™s old math with new computers."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#a-simple-example",
    "href": "posts/01-why-stats-matter/index.html#a-simple-example",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "A simple example",
    "text": "A simple example\nLetâ€™s say you want to predict house prices. A machine learning tutorial might tell you to throw your data into XGBoost. But whatâ€™s XGBoost actually doing?\nAt its core, itâ€™s fitting a sequence of simple models (like decision trees) where each new model tries to fix the mistakes of the previous ones. The â€œmistakesâ€ are measured using a loss function â€” which is a statistical concept. The way it finds the best split in a tree? Variance reduction â€” another statistical concept.\nLetâ€™s see this with the simplest possible model â€” a straight line:\n\n\nCode\n# Generate some example data\nset.seed(42)\nn &lt;- 50\nsquare_metres &lt;- runif(n, 40, 200)\nprice &lt;- 50000 + 2500 * square_metres + rnorm(n, 0, 25000)\n\n# Fit the simplest model\nmodel &lt;- lm(price ~ square_metres)\n\n# Plot\nplot(square_metres, price / 1000,\n     pch = 19, col = \"#1a1a2e90\",\n     xlab = \"Size (square metres)\",\n     ylab = \"Price (â‚¬ thousands)\",\n     main = \"House Prices: One Line Explains a Lot\",\n     family = \"sans\", cex.main = 1.2)\nabline(model$coefficients[1] / 1000, model$coefficients[2] / 1000,\n       col = \"#e94560\", lwd = 3)\n\n\n\n\n\n\n\n\nFigureÂ 1: A simple linear regression â€” the foundation beneath complex models\n\n\n\n\n\nThat red line? Thatâ€™s y = mx + b â€” something you learned in school. Itâ€™s also the building block of neural networks with millions of parameters. The only difference is scale."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#what-this-blog-will-cover",
    "href": "posts/01-why-stats-matter/index.html#what-this-blog-will-cover",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "What this blog will cover",
    "text": "What this blog will cover\nOver the coming weeks, weâ€™ll build your statistical intuition from the ground up. Each post will focus on one concept, explained so that:\n\nYou understand the why, not just the how\nYou can explain it to someone at a dinner party\nYou see how it connects to the AI/ML topics you care about\n\n\n\n\n\n\n\nTipKey Takeaway\n\n\n\nMachine learning is statistics at scale. Understanding the foundations doesnâ€™t slow you down â€” itâ€™s what separates someone who uses tools from someone who builds with them."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#coming-next-week",
    "href": "posts/01-why-stats-matter/index.html#coming-next-week",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "Coming next week",
    "text": "Coming next week\nWhat Data Actually Is â€” types, distributions, and why the shape of your data matters more than the algorithm you choose.\n\nFound this useful? Share it with someone whoâ€™s jumping into ML without the foundations. And follow along â€” we publish every week."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stats Beneath",
    "section": "",
    "text": "For ML engineers and data scientists who want to understand the mathematics driving their models â€” not just tune the hyperparameters. Intuition first, code second.\n\n\nNew article every week."
  },
  {
    "objectID": "index.html#latest-articles",
    "href": "index.html#latest-articles",
    "title": "Stats Beneath",
    "section": "Latest Articles",
    "text": "Latest Articles\n\n\n\n\n\nRandom Variables and Distributions: What Your Data Actually Is\n\n\n\nfoundations\n\nprobability\n\n\n\nYour dataset isnâ€™t just numbers in a CSV. Itâ€™s a realisation of random variables drawn from an unknown distribution. Until you understand that, every model you build is a guess about a process you havenâ€™t described.\n\n\n\n\n\nFebruary 13, 2026\n\n\n\n\n\n\n\nWhy Statistics Matters Before You Touch Machine Learning\n\n\n\nfoundations\n\nmachine-learning\n\n\n\nEveryone wants to build AI. But the real superpower isnâ€™t knowing which algorithm to use â€” itâ€™s understanding why it works.\n\n\n\n\n\nFebruary 10, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#topics-were-building-toward",
    "href": "index.html#topics-were-building-toward",
    "title": "Stats Beneath",
    "section": "Topics weâ€™re building toward",
    "text": "Topics weâ€™re building toward\n\nBias-Variance Tradeoff PCA & Eigendecomposition Maximum Likelihood Bayesian Inference Hypothesis Testing for ML GLMs Causal Inference"
  },
  {
    "objectID": "index.html#one-statistical-concept-explained-clearly-every-week",
    "href": "index.html#one-statistical-concept-explained-clearly-every-week",
    "title": "Stats Beneath",
    "section": "One statistical concept, explained clearly, every week",
    "text": "One statistical concept, explained clearly, every week\nJoin ML engineers and data scientists building the mathematical intuition their bootcamp skipped.\n\n\n Subscribe\n\n\nNo spam. Unsubscribe anytime."
  },
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheat Sheets",
    "section": "",
    "text": "Bookmark this page. When you forget what a p-value means at 2am before a deadline, these one-page summaries will save you.\nEach cheat sheet maps to an article in the Learning Path. New sheets are added as articles are published.\n\n\n\nğŸ“Š\n\n\nDescriptive Statistics\nMean, median, mode, variance, standard deviation â€” the essential summary numbers and when to use each.\nComing with Week 3 article\n\n\n\n\nğŸ””\n\n\nDistributions\nNormal, binomial, Poisson, uniform â€” what they look like, when they appear, and their key properties.\nComing with Week 5 article\n\n\n\n\nğŸ²\n\n\nProbability Rules\nAddition rule, multiplication rule, conditional probability, Bayesâ€™ theorem â€” all on one page.\nComing with Week 6 article\n\n\n\n\nğŸ§ª\n\n\nHypothesis Testing\nNull vs alternative, Type I & II errors, p-values, confidence intervals â€” the decision framework.\nComing with Week 8 article\n\n\n\n\nğŸ“ˆ\n\n\nRegression\nSimple & multiple linear regression, coefficients, R-squared, residuals, assumptions â€” the essentials.\nComing with Week 10 article\n\n\n\n\nğŸ§®\n\n\nMatrix Operations\nTranspose, multiplication, inverse, determinant â€” the linear algebra operations that power ML.\nComing with Week 11 article\n\n\n\n\n\nHow to Use These\nEach cheat sheet is designed to be printed on a single page or saved as a PDF. Theyâ€™re intentionally minimal â€” if you need the full explanation, click through to the linked article.\n\n\n\n\n\n\nTipSuggestion\n\n\n\nSave these to your phone or pin them above your desk. The best reference is the one you can access in 3 seconds.\n\n\n\nWant a cheat sheet on a specific topic? Let us know."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "G\n\n\n\n\nMSc Health Data Science Â· Bayesian Modelling Â· Statistical Foundations for ML"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "About me",
    "text": "About me\nIâ€™m a Health Data Science MSc student at the University of Galway, originally from Zimbabwe. I hold a First Class Honours degree in Operations Research and Statistics, and have spent years working as a Monitoring & Evaluation Specialist in the health sector, where I developed a deep appreciation for rigorous data analysis and evidence-based decision making.\nIn 2023, I was selected for the Ireland Fellows Programme, which brought me to Ireland to pursue advanced studies in health data science. My journey through operations research, public health program evaluation, and now Bayesian statistical modelling has taught me that the real power of data science lies not in knowing which algorithm to run, but in understanding why it works.\nI created Stats Beneath because Iâ€™ve seen too many talented engineers and data scientists skip over the statistical foundations that make machine learning actually work. This blog is my attempt to bridge that gap â€” explaining one statistical concept at a time, with clarity, intuition, and real-world context."
  },
  {
    "objectID": "about.html#what-im-working-on",
    "href": "about.html#what-im-working-on",
    "title": "About",
    "section": "What Iâ€™m working on",
    "text": "What Iâ€™m working on\nMSc Thesis: Iâ€™m currently working on small area estimation for malaria prevalence in Ghana using Bayesian hierarchical models. Specifically, Iâ€™m implementing BYM2 and ICAR spatial models in Stan to produce district-level prevalence estimates from survey data. This work combines my interests in public health, spatial statistics, and Bayesian inference.\nStats Beneath: This blog, where I publish weekly deep dives into the statistical concepts that power machine learning and AI. Each article starts with intuition, builds to mathematical understanding, and ends with practical takeaways.\nBayesian Explorer: An interactive Shiny application I built to help students and practitioners develop intuition for Bayesian concepts. It visualizes prior-likelihood-posterior relationships and lets you explore how different priors affect inference in real-time. Try it here â†’"
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About",
    "section": "Technical skills",
    "text": "Technical skills\nI work primarily in R for statistical analysis and modeling, using Stan and JAGS for Bayesian inference. I build interactive tools with Shiny, write and publish technical content with Quarto, and handle version control with Git. I also use Python for machine learning workflows and have experience with geospatial analysis (spatial statistics, mapping, GIS workflows).\nMy approach is tool-agnostic but principles-first: I believe understanding the statistical foundations makes you better at choosing the right tool for the job."
  },
  {
    "objectID": "about.html#connect",
    "href": "about.html#connect",
    "title": "About",
    "section": "Connect",
    "text": "Connect\nIâ€™m always interested in connecting with people working at the intersection of statistics, machine learning, and public health.\n\n\nGitHub â€” code, projects, and open-source contributions\nLinkedIn â€” professional background and updates\nEmail â€” reach out directly\n\n\n\nThis site is built with Quarto and deployed on GitHub Pages. All content is written in R Markdown and rendered to HTML. The source code is available on GitHub."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "All Articles",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nCategories\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nFebruary 13, 2026\n\n\nRandom Variables and Distributions: What Your Data Actually Is\n\n\nfoundations, probability\n\n\n10 min\n\n\n\n\n\n\nFebruary 10, 2026\n\n\nWhy Statistics Matters Before You Touch Machine Learning\n\n\nfoundations, machine-learning\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "did-you-know.html",
    "href": "did-you-know.html",
    "title": "Did You Know?",
    "section": "",
    "text": "Statistics isnâ€™t dry â€” itâ€™s full of wild, counterintuitive, and fascinating stories. Here are some of our favourites. We add new ones regularly."
  },
  {
    "objectID": "did-you-know.html#mind-bending-stats",
    "href": "did-you-know.html#mind-bending-stats",
    "title": "Did You Know?",
    "section": "Mind-Bending Stats",
    "text": "Mind-Bending Stats\n\n\nThe Birthday Paradox\n\nIn a room of just 23 people, thereâ€™s a 50% chance two of them share a birthday. With 70 people, the probability jumps to 99.9%. Our brains are terrible at probability â€” and thatâ€™s exactly why we need statistics.\n\n\n\nSurvivorship Bias\n\nDuring WWII, the military wanted to add armour to bomber planes. They studied returning planes and saw bullet holes clustered on the wings and fuselage. Mathematician Abraham Wald said: â€œArmour the places with NO holes.â€ The planes hit there never came back. This is survivorship bias â€” we only see the data that survived.\n\n\n\nSimpsonâ€™s Paradox\n\nA trend that appears in several groups of data can reverse when the groups are combined. UC Berkeley was once sued for gender discrimination in admissions. Overall, fewer women were admitted. But department by department, women were admitted at equal or higher rates. The â€œbiasâ€ was that women applied to more competitive departments.\n\n\n\nThe Monty Hall Problem\n\nYouâ€™re on a game show with 3 doors. Behind one is a car. You pick door 1. The host opens door 3 (a goat). Should you switch? Yes â€” switching gives you a 2/3 chance of winning. Even Paul ErdÅ‘s, one of the greatest mathematicians ever, refused to believe this until he saw a computer simulation."
  },
  {
    "objectID": "did-you-know.html#data-science-by-the-numbers",
    "href": "did-you-know.html#data-science-by-the-numbers",
    "title": "Did You Know?",
    "section": "Data Science by the Numbers",
    "text": "Data Science by the Numbers\n\n\n\n80%\n\n\nOf a data scientistâ€™s time is spent on data cleaning and preparation\n\n\n\n\n1.1T MB\n\n\nOf data generated daily worldwide\n\n\n\n\n70%\n\n\nOf data science projects never make it to production\n\n\n\n\n1763\n\n\nYear Bayes published his theorem â€” now powering spam filters and medical AI\n\n\n\n\n90%\n\n\nOf the worldâ€™s data was created in the last two years\n\n\n\n\n$100K+\n\n\nAverage data scientist salary (USD) â€” one of the highest-paid tech roles"
  },
  {
    "objectID": "did-you-know.html#quotes-worth-remembering",
    "href": "did-you-know.html#quotes-worth-remembering",
    "title": "Did You Know?",
    "section": "Quotes Worth Remembering",
    "text": "Quotes Worth Remembering\n\nâ€œAll models are wrong, but some are useful.â€ â€” George Box, statistician\n\n\nâ€œWithout data, youâ€™re just another person with an opinion.â€ â€” W. Edwards Deming\n\n\nâ€œThe best thing about being a statistician is that you get to play in everyoneâ€™s backyard.â€ â€” John Tukey\n\n\nâ€œFar better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question.â€ â€” John Tukey\n\n\nâ€œTorture the data, and it will confess to anything.â€ â€” Ronald Coase, economist"
  },
  {
    "objectID": "did-you-know.html#historical-firsts",
    "href": "did-you-know.html#historical-firsts",
    "title": "Did You Know?",
    "section": "Historical Firsts",
    "text": "Historical Firsts\n\n\nThe First Dataset\n\nJohn Graunt published the first known statistical analysis in 1662 â€” â€œNatural and Political Observations Made upon the Bills of Mortality.â€ He analysed London death records to estimate population, life expectancy, and disease patterns. Data science before computers existed.\n\n\n\nFlorence Nightingale â€” Data Viz Pioneer\n\nBefore she was known for nursing, Florence Nightingale was a statistician. Her polar area diagrams (a type of pie chart she invented) convinced the British government that more soldiers were dying from poor hospital conditions than from battle wounds. Data visualisation literally saved lives.\n\n\n\nThe Term â€œData Scientistâ€\n\nThe term â€œdata scientistâ€ was coined in 2008 by DJ Patil and Jeff Hammerbacher, who were building data teams at LinkedIn and Facebook. But the work â€” applying statistics to real problems â€” has been happening for centuries.\n\n\nKnow a great stats fact? Get in touch â€” weâ€™d love to feature it."
  },
  {
    "objectID": "learning-path.html",
    "href": "learning-path.html",
    "title": "Learning Path",
    "section": "",
    "text": "Not sure where to start? Follow this path. Each step builds on the last, and by the end youâ€™ll have the statistical foundations to truly understand machine learning and AI â€” not just run the code."
  },
  {
    "objectID": "learning-path.html#phase-1-the-basics",
    "href": "learning-path.html#phase-1-the-basics",
    "title": "Learning Path",
    "section": "Phase 1: The Basics",
    "text": "Phase 1: The Basics\nWhat every data-curious person should know.\n\n\n\nWeek 1\n\n\nWhy Statistics Matters Before You Touch ML\n\n\nThe case for learning foundations first. Why the best ML engineers are closet statisticians.\n\n\n\n\nWeek 2\n\n\nWhat Data Actually Is\n\n\nTypes, distributions, and why the shape of your data matters more than the algorithm you pick. Coming soon.\n\n\n\n\nWeek 3\n\n\nMean, Median, Mode â€” When Averages Lie\n\n\nThe three ways to measure â€œthe middleâ€ and when each one deceives you. Coming soon.\n\n\n\n\nWeek 4\n\n\nVariance & Standard Deviation\n\n\nMeasuring spread â€” why â€œclose to the averageâ€ means nothing without context. Coming soon."
  },
  {
    "objectID": "learning-path.html#phase-2-probability-distributions",
    "href": "learning-path.html#phase-2-probability-distributions",
    "title": "Learning Path",
    "section": "Phase 2: Probability & Distributions",
    "text": "Phase 2: Probability & Distributions\nThe language of uncertainty that every model speaks.\n\n\n\nWeek 5\n\n\nThe Normal Distribution\n\n\nWhy the bell curve shows up everywhere â€” from exam scores to stock prices. Coming soon.\n\n\n\n\nWeek 6\n\n\nProbability Basics\n\n\nConditional probability, Bayesâ€™ theorem, and why your intuition about probability is almost always wrong. Coming soon."
  },
  {
    "objectID": "learning-path.html#phase-3-inference-testing",
    "href": "learning-path.html#phase-3-inference-testing",
    "title": "Learning Path",
    "section": "Phase 3: Inference & Testing",
    "text": "Phase 3: Inference & Testing\nMaking conclusions from incomplete data â€” the heart of statistics.\n\n\n\nWeek 7\n\n\nSampling & Why Your Sample Isnâ€™t the Population\n\n\nWhy polls get elections wrong and what that teaches us about data. Coming soon.\n\n\n\n\nWeek 8\n\n\nHypothesis Testing & P-Values\n\n\nThe most misunderstood concept in all of science, explained properly. Coming soon."
  },
  {
    "objectID": "learning-path.html#phase-4-relationships-models",
    "href": "learning-path.html#phase-4-relationships-models",
    "title": "Learning Path",
    "section": "Phase 4: Relationships & Models",
    "text": "Phase 4: Relationships & Models\nFrom understanding data to predicting with it.\n\n\n\nWeek 9\n\n\nCorrelation â‰  Causation\n\n\nWhat correlation actually measures, why ice cream doesnâ€™t cause drowning, and how to think about relationships in data. Coming soon.\n\n\n\n\nWeek 10\n\n\nLinear Regression Explained\n\n\nFitting a line through data â€” the single most important technique in all of statistics. Coming soon.\n\n\n\n\nWeek 11\n\n\nMatrices â€” Why They Power Everything\n\n\nLinear algebra for humans. How your data becomes a matrix and why that matters. Coming soon.\n\n\n\n\nWeek 12\n\n\nFrom Regression to Machine Learning\n\n\nConnecting the dots â€” how everything youâ€™ve learned maps directly to ML algorithms. Coming soon.\n\n\n\n\nNew articles published weekly. Links will become active as each post goes live."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html",
    "href": "posts/02-random-variables-distributions/index.html",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "",
    "text": "Open any ML tutorial. Step one is always the same: load the data. pd.read_csv('data.csv'). You get a DataFrame. Rows and columns. Numbers.\nAnd then you do things to those numbers. Scale them. Split them. Feed them to a model. Tune hyperparameters. Evaluate. Ship.\nBut hereâ€™s the question almost nobody stops to ask: where did those numbers come from?\nNot â€œwhich APIâ€ or â€œwhich database.â€ I mean: what process generated them? Is each row independent of the others? Could the values have turned out differently if youâ€™d collected data on a different day? If you collected more data tomorrow, would the new rows look like the old ones?\nThese arenâ€™t philosophical questions. Theyâ€™re statistical ones. And your answers to them â€” whether you state them explicitly or not â€” determine whether your modelâ€™s predictions mean anything at all.\nThe language for thinking about this clearly is random variables and probability distributions. If youâ€™ve seen these terms in a textbook and moved on, Iâ€™d like to show you why theyâ€™re not abstract theory â€” theyâ€™re the precise description of what your data is and what your model is trying to learn."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#your-data-has-a-secret-life",
    "href": "posts/02-random-variables-distributions/index.html#your-data-has-a-secret-life",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "",
    "text": "Open any ML tutorial. Step one is always the same: load the data. pd.read_csv('data.csv'). You get a DataFrame. Rows and columns. Numbers.\nAnd then you do things to those numbers. Scale them. Split them. Feed them to a model. Tune hyperparameters. Evaluate. Ship.\nBut hereâ€™s the question almost nobody stops to ask: where did those numbers come from?\nNot â€œwhich APIâ€ or â€œwhich database.â€ I mean: what process generated them? Is each row independent of the others? Could the values have turned out differently if youâ€™d collected data on a different day? If you collected more data tomorrow, would the new rows look like the old ones?\nThese arenâ€™t philosophical questions. Theyâ€™re statistical ones. And your answers to them â€” whether you state them explicitly or not â€” determine whether your modelâ€™s predictions mean anything at all.\nThe language for thinking about this clearly is random variables and probability distributions. If youâ€™ve seen these terms in a textbook and moved on, Iâ€™d like to show you why theyâ€™re not abstract theory â€” theyâ€™re the precise description of what your data is and what your model is trying to learn."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#a-random-variable-is-a-question-not-an-answer",
    "href": "posts/02-random-variables-distributions/index.html#a-random-variable-is-a-question-not-an-answer",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "A random variable is a question, not an answer",
    "text": "A random variable is a question, not an answer\nHereâ€™s the definition youâ€™ll find everywhere: a random variable is a function that maps outcomes from a sample space to real numbers.\nThatâ€™s technically correct and practically useless. Let me give you a better one.\nA random variable is a numerical quantity whose value hasnâ€™t been determined yet.\nBefore you observe your data, you donâ€™t know what values youâ€™ll get. Will this patientâ€™s blood pressure be 120 or 145? Will this customer churn or stay? Will this image contain a cat? You donâ€™t know. But you know the kind of values that are possible, and you may have beliefs about which values are more likely.\nThatâ€™s what a random variable captures: the space of possible values and their relative likelihoods, before you observe anything.\nOnce you observe a value â€” once the patientâ€™s blood pressure reads 132 â€” thatâ€™s no longer a random variable. Itâ€™s a realisation. A data point. One draw from the underlying random process.\nYour entire dataset is a collection of realisations. Each row in your DataFrame was, before you observed it, a random variable. Now itâ€™s a fixed number. But the process that generated it is still out there, and it could generate different numbers tomorrow.\nThis is why it matters: your model isnâ€™t trying to memorise your specific 10,000 rows. Itâ€™s trying to learn the process that generated them, so it can make predictions about rows it hasnâ€™t seen yet. You canâ€™t do that without thinking about your data as draws from something larger.\n\nThe notation\nStatisticians use uppercase letters for random variables and lowercase for their observed values:\n\n\\(X\\) = the random variable (the question: â€œwhat will this patientâ€™s blood pressure be?â€)\n\\(x\\) = an observed value, a realisation (the answer: 132)\n\nWhen we write \\(X = x\\), we mean â€œthe random variable \\(X\\) took the value \\(x\\).â€ When we write \\(P(X = x)\\), we mean â€œthe probability that \\(X\\) takes the value \\(x\\).â€\nThis isnâ€™t pedantry. The distinction between \\(X\\) and \\(x\\) is the distinction between the process and the data. Confusing them is how you end up overfitting."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#distributions-the-shape-of-uncertainty",
    "href": "posts/02-random-variables-distributions/index.html#distributions-the-shape-of-uncertainty",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "Distributions: the shape of uncertainty",
    "text": "Distributions: the shape of uncertainty\nIf a random variable describes what could happen, a probability distribution describes how likely each possibility is.\nThink of it as a contract. Before you observe any data, the distribution tells you: â€œHere are all the possible values, and hereâ€™s the relative chance of each one.â€\nThere are two flavours, depending on whether the random variable takes countable values or values on a continuum.\n\nDiscrete distributions: counting outcomes\nA discrete random variable takes values you can list: 0, 1, 2, 3, â€¦ or {cat, dog, bird}. The probability of each value is given by a probability mass function (PMF):\n\\[P(X = x) = p(x)\\]\nwith two rules: every probability is between 0 and 1, and they all add up to 1.\nThe Bernoulli distribution is the simplest possible distribution. A single trial. Two outcomes. Probability \\(p\\) of success, \\(1-p\\) of failure.\n\\[X \\sim \\text{Bernoulli}(p), \\quad P(X=1) = p, \\quad P(X=0) = 1-p\\]\nEvery binary classification target in your dataset follows a Bernoulli distribution. When your logistic regression outputs \\(\\hat{p} = 0.73\\), itâ€™s estimating the parameter of a Bernoulli distribution. Thatâ€™s literally what itâ€™s doing â€” fitting \\(p\\).\nThe Binomial distribution counts the number of successes in \\(n\\) independent Bernoulli trials:\n\\[X \\sim \\text{Binomial}(n, p), \\quad P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nIf you flip a coin 100 times, the number of heads is Binomial. If you classify 100 samples and count the correct ones, the number correct is Binomial (under independence). This is why your modelâ€™s accuracy has a sampling distribution â€” itâ€™s not a fixed number, itâ€™s a draw from a Binomial.\nThe Poisson distribution models the count of events in a fixed interval when events happen at a constant average rate:\n\\[X \\sim \\text{Poisson}(\\lambda), \\quad P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\]\nServer requests per minute. Customer complaints per day. Gene mutations per chromosome. Any time youâ€™re modelling â€œhow many times does something happen,â€ youâ€™re probably looking at Poisson data.\n\n\nContinuous distributions: measuring on a continuum\nA continuous random variable takes values on the real line (or an interval). You canâ€™t list all possible values, so you canâ€™t assign a probability to any single value â€” \\(P(X = 1.23456789...) = 0\\) for any specific number.\nInstead, you describe probabilities over intervals using a probability density function (PDF):\n\\[P(a \\leq X \\leq b) = \\int_a^b f(x)\\,dx\\]\nThe PDF \\(f(x)\\) tells you how densely the probability is packed around each value. Itâ€™s not a probability itself â€” it can be greater than 1 â€” but the area under the curve over any interval gives you a probability.\nThe Normal (Gaussian) distribution is the one you know:\n\\[X \\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\]\nTwo parameters: \\(\\mu\\) (the centre) and \\(\\sigma^2\\) (the spread). It shows up everywhere, and thereâ€™s a deep reason for that â€” the Central Limit Theorem, which weâ€™ll cover in a future article.\nBut hereâ€™s what matters for ML: when you assume your regression errors are normally distributed, youâ€™re saying the residuals follow this specific shape. Thatâ€™s not a trivial assumption. It determines your loss function (MSE corresponds to Gaussian errors), your confidence intervals, and your hypothesis tests. If the assumption is wrong, all of those break.\nThe Uniform distribution assigns equal probability to all values in an interval:\n\\[X \\sim \\text{Uniform}(a, b), \\quad f(x) = \\frac{1}{b-a} \\quad \\text{for } a \\leq x \\leq b\\]\nThis is the distribution of â€œI have no idea what value to expect.â€ When you initialise neural network weights uniformly, youâ€™re sampling from this distribution. When you use random search for hyperparameter tuning, youâ€™re drawing from it. Itâ€™s the mathematical formalisation of ignorance."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#the-cdf-the-cumulative-view",
    "href": "posts/02-random-variables-distributions/index.html#the-cdf-the-cumulative-view",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "The CDF: the cumulative view",
    "text": "The CDF: the cumulative view\nThe cumulative distribution function (CDF) gives the probability that \\(X\\) is less than or equal to a value:\n\\[F(x) = P(X \\leq x)\\]\nFor discrete random variables, the CDF is a step function. For continuous ones, itâ€™s a smooth curve from 0 to 1. The CDF always exists, even when the PDF doesnâ€™t (for discrete variables), which makes it the more fundamental object.\nWhy should you care? Because when you compute a percentile, a quantile, or a p-value, youâ€™re using the CDF. When you say â€œthis patientâ€™s blood pressure is in the 95th percentile,â€ youâ€™re saying \\(F(x) = 0.95\\)."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#the-connection-to-ml-you-probably-missed",
    "href": "posts/02-random-variables-distributions/index.html#the-connection-to-ml-you-probably-missed",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "The connection to ML you probably missed",
    "text": "The connection to ML you probably missed\nHereâ€™s where everything clicks.\nEvery supervised learning problem is an attempt to learn a conditional distribution.\nWhen you fit a regression model, youâ€™re estimating:\n\\[P(Y \\mid X) = \\text{some distribution parameterised by } X\\]\nFor linear regression with Gaussian errors, this is:\n\\[Y \\mid X \\sim \\mathcal{N}(X\\beta, \\sigma^2)\\]\nFor logistic regression:\n\\[Y \\mid X \\sim \\text{Bernoulli}(\\sigma(X\\beta))\\]\nFor Poisson regression:\n\\[Y \\mid X \\sim \\text{Poisson}(\\exp(X\\beta))\\]\nIn each case, \\(X\\) (the features) determine the parameters of the distribution, and \\(Y\\) (the target) is a random draw from that distribution. The model doesnâ€™t predict \\(Y\\) directly â€” it predicts the distribution of \\(Y\\), and the point prediction is just a summary (usually the mean).\nThis is why understanding distributions isnâ€™t optional for ML. Youâ€™re fitting them whether you know it or not. sklearn just hides it from you.\n\nThe loss function is the distribution\nThis connection goes even deeper. Your choice of loss function implicitly assumes a distribution:\n\n\n\nLoss Function\nImplicit Distribution\nLink\n\n\n\n\nMean Squared Error\nNormal (Gaussian)\nIdentity\n\n\nCross-Entropy\nBernoulli / Categorical\nLogit / Softmax\n\n\nPoisson Deviance\nPoisson\nLog\n\n\nHuber Loss\nA compromise: Normal near 0, Laplace in the tails\nâ€”\n\n\n\nWhen you minimise MSE, youâ€™re doing maximum likelihood estimation under the assumption that your errors are Gaussian. When you minimise cross-entropy, youâ€™re doing MLE under the assumption that your targets are Bernoulli.\nIf youâ€™ve ever wondered â€œwhy MSE for regression and cross-entropy for classification?â€ â€” this is the answer. Itâ€™s not arbitrary. Each loss function is derived from a distributional assumption about the target variable. Weâ€™ll make this precise when we cover likelihood in a future article."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#what-this-means-in-practice",
    "href": "posts/02-random-variables-distributions/index.html#what-this-means-in-practice",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "What this means in practice",
    "text": "What this means in practice\nUnderstanding random variables and distributions gives you three practical superpowers:\n1. You can diagnose model failures. If your regression residuals arenâ€™t approximately normal, MSE might not be the right loss. If your count data has more zeros than Poisson allows, you need a zero-inflated model. If your binary classifierâ€™s predicted probabilities donâ€™t match observed frequencies, itâ€™s miscalibrated. All of these diagnoses require understanding the distributional assumptions youâ€™ve made.\n2. You can quantify uncertainty. A prediction of \\(\\hat{y} = 42\\) is useless without knowing how confident you are. If you know the distribution, you can compute prediction intervals, confidence intervals, and posterior distributions. Without it, youâ€™re flying blind.\n3. You can choose the right model. Gaussian targets â†’ linear regression (or ridge, lasso). Binary targets â†’ logistic regression. Count targets â†’ Poisson regression. Skewed positive targets â†’ Gamma regression. Bounded proportions â†’ Beta regression. The distribution of your target variable tells you which model family to use."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#the-mental-model-to-take-away",
    "href": "posts/02-random-variables-distributions/index.html#the-mental-model-to-take-away",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "The mental model to take away",
    "text": "The mental model to take away\nHereâ€™s the shift I want you to make:\nBefore: â€œI have data in a CSV. Iâ€™ll fit a model to it.â€\nAfter: â€œMy data is a sample of realisations from an unknown data-generating process. My model is a hypothesis about what that process looks like. Training is estimating the parameters of that process. Evaluation is checking whether my hypothesis is consistent with new realisations.â€\nThat second framing is what statistics gives you. Itâ€™s more precise, more honest, and it leads to better models.\nNext week, weâ€™ll build on this foundation. You now know that your data comes from distributions and your models estimate distributions. But what exactly are you estimating when you compute a mean, a variance, or a loss? Thatâ€™s the world of expected values â€” and it turns out your loss function is one.\n\nThis is article 2 of Stats Beneath, a weekly series on the statistical foundations of machine learning. Subscribe to get each article when itâ€™s published."
  }
]