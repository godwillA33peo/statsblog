[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Everything here is something Iâ€™ve personally used, read, or can vouch for. No affiliate links â€” just resources I genuinely think are worth your time."
  },
  {
    "objectID": "resources.html#books",
    "href": "resources.html#books",
    "title": "Resources",
    "section": "Books",
    "text": "Books\n\n\nğŸ“• The Art of Statistics â€” David Spiegelhalter\nThe best â€œstats for everyoneâ€ book. Spiegelhalter uses real stories to explain statistical thinking without equations. Start here if youâ€™re brand new.\n\n\nğŸ“— Naked Statistics â€” Charles Wheelan\nFunny, accessible, and packed with real-world examples. Great for understanding why statistics matters in everyday life.\n\n\nğŸ“˜ An Introduction to Statistical Learning (ISLR) â€” James, Witten, Hastie, Tibshirani\nThe bridge between statistics and machine learning. Free PDF available at statlearning.com. The R labs are excellent.\n\n\nğŸ“™ Statistical Rethinking â€” Richard McElreath\nIf you want to understand Bayesian statistics properly, this is the gold standard. Challenging but rewarding, with amazing R and Stan code examples."
  },
  {
    "objectID": "resources.html#free-courses",
    "href": "resources.html#free-courses",
    "title": "Resources",
    "section": "Free Courses",
    "text": "Free Courses\n\n\nğŸ“ Khan Academy â€” Statistics & Probability\nVisit â†’\nThe best free foundation. Clear videos, practice exercises, covers everything from basics to inference.\n\n\nğŸ“ StatQuest (YouTube)\nVisit â†’\nJosh Starmer explains statistics and ML with simple visuals and catchy jingles. Surprisingly effective.\n\n\nğŸ“ Harvard CS109 â€” Data Science\nVisit â†’\nFull Harvard course with lectures, labs, and assignments. Bridges stats and practical data science beautifully.\n\n\nğŸ“ Seeing Theory\nVisit â†’\nInteractive visualisations of probability and statistics concepts. Built by Brown University. Beautiful and educational."
  },
  {
    "objectID": "resources.html#tools-we-use",
    "href": "resources.html#tools-we-use",
    "title": "Resources",
    "section": "Tools We Use",
    "text": "Tools We Use\n\n\nğŸ”§ R & Positron\nposit.co/positron\nOur language and IDE of choice. R is unmatched for statistical computing, and Positron is a modern, fast editor built for data science.\n\n\nğŸ”§ Quarto\nquarto.org\nWhat this site is built with. Write documents, blogs, and presentations that mix text, code, and output.\n\n\nğŸ”§ Stan\nmc-stan.org\nThe gold standard for Bayesian modeling. Steep learning curve, but incredibly powerful for serious statistical work.\n\n\nğŸ”§ Shiny\nshiny.posit.co\nBuild interactive web apps from R. Great for creating data explorations and teaching tools."
  },
  {
    "objectID": "resources.html#communities",
    "href": "resources.html#communities",
    "title": "Resources",
    "section": "Communities",
    "text": "Communities\n\n\nğŸ’¬ Cross Validated (Stack Exchange)\nVisit â†’\nThe best Q&A site for statistics. Search before you ask â€” most common questions have excellent answers already.\n\n\nğŸ’¬ R-bloggers\nVisit â†’\nAggregated blog posts from hundreds of R users. Great for discovering tutorials and new packages.\n\n\nğŸ’¬ Stan Forums\nVisit â†’\nIncredibly helpful community for Bayesian modelling questions. The developers themselves answer questions.\n\n\nğŸ’¬ Posit Community\nVisit â†’\nFor R, Shiny, Quarto, and Positron questions. Friendly and welcoming to beginners."
  },
  {
    "objectID": "resources.html#datasets-for-practice",
    "href": "resources.html#datasets-for-practice",
    "title": "Resources",
    "section": "Datasets for Practice",
    "text": "Datasets for Practice\n\n\nğŸ“¦ Tidy Tuesday\nVisit â†’\nWeekly datasets released for the R community. Great for practice with real, messy data.\n\n\nğŸ“¦ Kaggle Datasets\nVisit â†’\nThousands of datasets across every domain. Filter by â€œbeginner friendlyâ€ to start.\n\n\nğŸ“¦ UCI Machine Learning Repository\nVisit â†’\nClassic benchmark datasets used in academic research. The Iris and Wine datasets live here.\n\n\nğŸ“¦ Our World in Data\nVisit â†’\nBeautiful, well-documented global datasets on health, economics, education, and more. Perfect for practice with real-world context.\n\n\n\n\n\n\n\n\n\nTipSuggest a Resource\n\n\n\nKnow something that belongs here? Get in touch â€” weâ€™re always looking for quality recommendations."
  },
  {
    "objectID": "learning-path.html",
    "href": "learning-path.html",
    "title": "Learning Path",
    "section": "",
    "text": "Not sure where to start? Follow this path. Each step builds on the last, and by the end youâ€™ll have the statistical foundations to truly understand machine learning and AI â€” not just run the code."
  },
  {
    "objectID": "learning-path.html#phase-1-the-basics",
    "href": "learning-path.html#phase-1-the-basics",
    "title": "Learning Path",
    "section": "Phase 1: The Basics",
    "text": "Phase 1: The Basics\nWhat every data-curious person should know.\n\n\n\nWeek 1\n\n\nWhy Statistics Matters Before You Touch ML\n\n\nThe case for learning foundations first. Why the best ML engineers are closet statisticians.\n\n\n\n\nWeek 2\n\n\nWhat Data Actually Is\n\n\nTypes, distributions, and why the shape of your data matters more than the algorithm you pick. Coming soon.\n\n\n\n\nWeek 3\n\n\nMean, Median, Mode â€” When Averages Lie\n\n\nThe three ways to measure â€œthe middleâ€ and when each one deceives you. Coming soon.\n\n\n\n\nWeek 4\n\n\nVariance & Standard Deviation\n\n\nMeasuring spread â€” why â€œclose to the averageâ€ means nothing without context. Coming soon."
  },
  {
    "objectID": "learning-path.html#phase-2-probability-distributions",
    "href": "learning-path.html#phase-2-probability-distributions",
    "title": "Learning Path",
    "section": "Phase 2: Probability & Distributions",
    "text": "Phase 2: Probability & Distributions\nThe language of uncertainty that every model speaks.\n\n\n\nWeek 5\n\n\nThe Normal Distribution\n\n\nWhy the bell curve shows up everywhere â€” from exam scores to stock prices. Coming soon.\n\n\n\n\nWeek 6\n\n\nProbability Basics\n\n\nConditional probability, Bayesâ€™ theorem, and why your intuition about probability is almost always wrong. Coming soon."
  },
  {
    "objectID": "learning-path.html#phase-3-inference-testing",
    "href": "learning-path.html#phase-3-inference-testing",
    "title": "Learning Path",
    "section": "Phase 3: Inference & Testing",
    "text": "Phase 3: Inference & Testing\nMaking conclusions from incomplete data â€” the heart of statistics.\n\n\n\nWeek 7\n\n\nSampling & Why Your Sample Isnâ€™t the Population\n\n\nWhy polls get elections wrong and what that teaches us about data. Coming soon.\n\n\n\n\nWeek 8\n\n\nHypothesis Testing & P-Values\n\n\nThe most misunderstood concept in all of science, explained properly. Coming soon."
  },
  {
    "objectID": "learning-path.html#phase-4-relationships-models",
    "href": "learning-path.html#phase-4-relationships-models",
    "title": "Learning Path",
    "section": "Phase 4: Relationships & Models",
    "text": "Phase 4: Relationships & Models\nFrom understanding data to predicting with it.\n\n\n\nWeek 9\n\n\nCorrelation â‰  Causation\n\n\nWhat correlation actually measures, why ice cream doesnâ€™t cause drowning, and how to think about relationships in data. Coming soon.\n\n\n\n\nWeek 10\n\n\nLinear Regression Explained\n\n\nFitting a line through data â€” the single most important technique in all of statistics. Coming soon.\n\n\n\n\nWeek 11\n\n\nMatrices â€” Why They Power Everything\n\n\nLinear algebra for humans. How your data becomes a matrix and why that matters. Coming soon.\n\n\n\n\nWeek 12\n\n\nFrom Regression to Machine Learning\n\n\nConnecting the dots â€” how everything youâ€™ve learned maps directly to ML algorithms. Coming soon.\n\n\n\n\nNew articles published weekly. Links will become active as each post goes live."
  },
  {
    "objectID": "did-you-know.html",
    "href": "did-you-know.html",
    "title": "Did You Know?",
    "section": "",
    "text": "Statistics isnâ€™t dry â€” itâ€™s full of wild, counterintuitive, and fascinating stories. Here are some of our favourites. We add new ones regularly."
  },
  {
    "objectID": "did-you-know.html#mind-bending-stats",
    "href": "did-you-know.html#mind-bending-stats",
    "title": "Did You Know?",
    "section": "Mind-Bending Stats",
    "text": "Mind-Bending Stats\n\n\nThe Birthday Paradox\n\nIn a room of just 23 people, thereâ€™s a 50% chance two of them share a birthday. With 70 people, the probability jumps to 99.9%. Our brains are terrible at probability â€” and thatâ€™s exactly why we need statistics.\n\n\n\nSurvivorship Bias\n\nDuring WWII, the military wanted to add armour to bomber planes. They studied returning planes and saw bullet holes clustered on the wings and fuselage. Mathematician Abraham Wald said: â€œArmour the places with NO holes.â€ The planes hit there never came back. This is survivorship bias â€” we only see the data that survived.\n\n\n\nSimpsonâ€™s Paradox\n\nA trend that appears in several groups of data can reverse when the groups are combined. UC Berkeley was once sued for gender discrimination in admissions. Overall, fewer women were admitted. But department by department, women were admitted at equal or higher rates. The â€œbiasâ€ was that women applied to more competitive departments.\n\n\n\nThe Monty Hall Problem\n\nYouâ€™re on a game show with 3 doors. Behind one is a car. You pick door 1. The host opens door 3 (a goat). Should you switch? Yes â€” switching gives you a 2/3 chance of winning. Even Paul ErdÅ‘s, one of the greatest mathematicians ever, refused to believe this until he saw a computer simulation."
  },
  {
    "objectID": "did-you-know.html#data-science-by-the-numbers",
    "href": "did-you-know.html#data-science-by-the-numbers",
    "title": "Did You Know?",
    "section": "Data Science by the Numbers",
    "text": "Data Science by the Numbers\n\n\n\n80%\n\n\nOf a data scientistâ€™s time is spent on data cleaning and preparation\n\n\n\n\n1.1T MB\n\n\nOf data generated daily worldwide\n\n\n\n\n70%\n\n\nOf data science projects never make it to production\n\n\n\n\n1763\n\n\nYear Bayes published his theorem â€” now powering spam filters and medical AI\n\n\n\n\n90%\n\n\nOf the worldâ€™s data was created in the last two years\n\n\n\n\n$100K+\n\n\nAverage data scientist salary (USD) â€” one of the highest-paid tech roles"
  },
  {
    "objectID": "did-you-know.html#quotes-worth-remembering",
    "href": "did-you-know.html#quotes-worth-remembering",
    "title": "Did You Know?",
    "section": "Quotes Worth Remembering",
    "text": "Quotes Worth Remembering\n\nâ€œAll models are wrong, but some are useful.â€ â€” George Box, statistician\n\n\nâ€œWithout data, youâ€™re just another person with an opinion.â€ â€” W. Edwards Deming\n\n\nâ€œThe best thing about being a statistician is that you get to play in everyoneâ€™s backyard.â€ â€” John Tukey\n\n\nâ€œFar better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question.â€ â€” John Tukey\n\n\nâ€œTorture the data, and it will confess to anything.â€ â€” Ronald Coase, economist"
  },
  {
    "objectID": "did-you-know.html#historical-firsts",
    "href": "did-you-know.html#historical-firsts",
    "title": "Did You Know?",
    "section": "Historical Firsts",
    "text": "Historical Firsts\n\n\nThe First Dataset\n\nJohn Graunt published the first known statistical analysis in 1662 â€” â€œNatural and Political Observations Made upon the Bills of Mortality.â€ He analysed London death records to estimate population, life expectancy, and disease patterns. Data science before computers existed.\n\n\n\nFlorence Nightingale â€” Data Viz Pioneer\n\nBefore she was known for nursing, Florence Nightingale was a statistician. Her polar area diagrams (a type of pie chart she invented) convinced the British government that more soldiers were dying from poor hospital conditions than from battle wounds. Data visualisation literally saved lives.\n\n\n\nThe Term â€œData Scientistâ€\n\nThe term â€œdata scientistâ€ was coined in 2008 by DJ Patil and Jeff Hammerbacher, who were building data teams at LinkedIn and Facebook. But the work â€” applying statistics to real problems â€” has been happening for centuries.\n\n\nKnow a great stats fact? Get in touch â€” weâ€™d love to feature it."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "All Articles",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nCategories\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nFebruary 10, 2026\n\n\nWhy Statistics Matters Before You Touch Machine Learning\n\n\nfoundations, machine-learning\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Iâ€™m a Health Data Science MSc student at the University of Galway, with a background in Operations Research and Statistics. Iâ€™ve worked as a Monitoring & Evaluation Specialist in public health and currently spend my days fitting Bayesian models and wrangling survey data.\n\n\nIâ€™ve noticed something in the rush toward AI and machine learning: people want to run the algorithms but skip understanding why they work. The matrix algebra, the probability theory, the regression foundations, these arenâ€™t just academic hurdles. Theyâ€™re the difference between someone who uses tools and someone who understands them.\nStats Beneath is my attempt to bridge that gap. Each week, I take one statistical concept and explain it the way I wish it had been explained to me, with real-world analogies, clear visuals, and zero unnecessary jargon.\n\n\n\nEvery article follows a simple structure:\n\nThe real-world hook - why should you care?\nThe intuition - whatâ€™s actually going on, in plain language\nThe visual - a plot or diagram that makes it click\nThe code (optional) - simple R examples for those who want hands-on practice\nThe takeaway - one sentence youâ€™ll remember\n\nIf you find these useful, share them with someone who thinks statistics is scary. It doesnâ€™t have to be."
  },
  {
    "objectID": "about.html#hi-im-godwill",
    "href": "about.html#hi-im-godwill",
    "title": "About",
    "section": "",
    "text": "Iâ€™m a Health Data Science MSc student at the University of Galway, with a background in Operations Research and Statistics. Iâ€™ve worked as a Monitoring & Evaluation Specialist in public health and currently spend my days fitting Bayesian models and wrangling survey data.\n\n\nIâ€™ve noticed something in the rush toward AI and machine learning: people want to run the algorithms but skip understanding why they work. The matrix algebra, the probability theory, the regression foundations, these arenâ€™t just academic hurdles. Theyâ€™re the difference between someone who uses tools and someone who understands them.\nStats Beneath is my attempt to bridge that gap. Each week, I take one statistical concept and explain it the way I wish it had been explained to me, with real-world analogies, clear visuals, and zero unnecessary jargon.\n\n\n\nEvery article follows a simple structure:\n\nThe real-world hook - why should you care?\nThe intuition - whatâ€™s actually going on, in plain language\nThe visual - a plot or diagram that makes it click\nThe code (optional) - simple R examples for those who want hands-on practice\nThe takeaway - one sentence youâ€™ll remember\n\nIf you find these useful, share them with someone who thinks statistics is scary. It doesnâ€™t have to be."
  },
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheat Sheets",
    "section": "",
    "text": "Bookmark this page. When you forget what a p-value means at 2am before a deadline, these one-page summaries will save you.\nEach cheat sheet maps to an article in the Learning Path. New sheets are added as articles are published.\n\n\n\nğŸ“Š\n\n\nDescriptive Statistics\nMean, median, mode, variance, standard deviation â€” the essential summary numbers and when to use each.\nComing with Week 3 article\n\n\n\n\nğŸ””\n\n\nDistributions\nNormal, binomial, Poisson, uniform â€” what they look like, when they appear, and their key properties.\nComing with Week 5 article\n\n\n\n\nğŸ²\n\n\nProbability Rules\nAddition rule, multiplication rule, conditional probability, Bayesâ€™ theorem â€” all on one page.\nComing with Week 6 article\n\n\n\n\nğŸ§ª\n\n\nHypothesis Testing\nNull vs alternative, Type I & II errors, p-values, confidence intervals â€” the decision framework.\nComing with Week 8 article\n\n\n\n\nğŸ“ˆ\n\n\nRegression\nSimple & multiple linear regression, coefficients, R-squared, residuals, assumptions â€” the essentials.\nComing with Week 10 article\n\n\n\n\nğŸ§®\n\n\nMatrix Operations\nTranspose, multiplication, inverse, determinant â€” the linear algebra operations that power ML.\nComing with Week 11 article\n\n\n\n\n\nHow to Use These\nEach cheat sheet is designed to be printed on a single page or saved as a PDF. Theyâ€™re intentionally minimal â€” if you need the full explanation, click through to the linked article.\n\n\n\n\n\n\nTipSuggestion\n\n\n\nSave these to your phone or pin them above your desk. The best reference is the one you can access in 3 seconds.\n\n\n\nWant a cheat sheet on a specific topic? Let us know."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stats Beneath",
    "section": "",
    "text": "Statistics simplified â€” understanding the foundations that power AI, machine learning, and data science.\n\n\nNew article every week"
  },
  {
    "objectID": "index.html#latest-articles",
    "href": "index.html#latest-articles",
    "title": "Stats Beneath",
    "section": "Latest Articles",
    "text": "Latest Articles\nEach week, we break down one statistical concept with real-world analogies, clear visuals, and optional R code. No jargon walls. No assumptions about your background.\n\nStay in the loop\nOne email per week. One concept explained simply. No spam, no fluff.\nSubscribe â†’"
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html",
    "href": "posts/01-why-stats-matter/index.html",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "",
    "text": "Did you know?\n\nThe term â€œregressionâ€ comes from Francis Galtonâ€™s 1886 study on the heights of parents and children. He noticed that tall parents tended to have slightly shorter children, and short parents had slightly taller ones â€” they â€œregressedâ€ toward the average. The name stuck, even though modern regression has nothing to do with shrinking.\n\nThereâ€™s a pattern I see everywhere. Someone discovers machine learning, gets excited, installs scikit-learn or tidymodels, runs a random forest on a dataset, and gets 94% accuracy. Amazing! Ship it!\nBut then someone asks: â€œWhy did your model predict that?â€ And the answer isâ€¦ silence.\nThis is what happens when we skip the foundation. Machine learning is not magic â€” itâ€™s statistics at scale. And if you donâ€™t understand the statistics beneath the algorithm, youâ€™re building on sand."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#the-rush-to-the-algorithm",
    "href": "posts/01-why-stats-matter/index.html#the-rush-to-the-algorithm",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "",
    "text": "Did you know?\n\nThe term â€œregressionâ€ comes from Francis Galtonâ€™s 1886 study on the heights of parents and children. He noticed that tall parents tended to have slightly shorter children, and short parents had slightly taller ones â€” they â€œregressedâ€ toward the average. The name stuck, even though modern regression has nothing to do with shrinking.\n\nThereâ€™s a pattern I see everywhere. Someone discovers machine learning, gets excited, installs scikit-learn or tidymodels, runs a random forest on a dataset, and gets 94% accuracy. Amazing! Ship it!\nBut then someone asks: â€œWhy did your model predict that?â€ And the answer isâ€¦ silence.\nThis is what happens when we skip the foundation. Machine learning is not magic â€” itâ€™s statistics at scale. And if you donâ€™t understand the statistics beneath the algorithm, youâ€™re building on sand."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#what-beneath-actually-means",
    "href": "posts/01-why-stats-matter/index.html#what-beneath-actually-means",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "What â€œbeneathâ€ actually means",
    "text": "What â€œbeneathâ€ actually means\nEvery machine learning model youâ€™ve ever heard of is built on statistical concepts that have existed for decades:\nLinear regression (1800s) â†’ Neural networks are layers of linear regressions with activation functions stacked on top.\nBayesâ€™ theorem (1763) â†’ The entire field of Bayesian machine learning, spam filters, and medical diagnosis models.\nMatrix multiplication (1850s) â†’ Literally how your data flows through every neural network.\nProbability distributions (1700s) â†’ How models quantify uncertainty, make predictions, and learn from data.\nThe â€œAI revolutionâ€ isnâ€™t new math. Itâ€™s old math with new computers."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#a-simple-example",
    "href": "posts/01-why-stats-matter/index.html#a-simple-example",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "A simple example",
    "text": "A simple example\nLetâ€™s say you want to predict house prices. A machine learning tutorial might tell you to throw your data into XGBoost. But whatâ€™s XGBoost actually doing?\nAt its core, itâ€™s fitting a sequence of simple models (like decision trees) where each new model tries to fix the mistakes of the previous ones. The â€œmistakesâ€ are measured using a loss function â€” which is a statistical concept. The way it finds the best split in a tree? Variance reduction â€” another statistical concept.\nLetâ€™s see this with the simplest possible model â€” a straight line:\n\n\nCode\n# Generate some example data\nset.seed(42)\nn &lt;- 50\nsquare_metres &lt;- runif(n, 40, 200)\nprice &lt;- 50000 + 2500 * square_metres + rnorm(n, 0, 25000)\n\n# Fit the simplest model\nmodel &lt;- lm(price ~ square_metres)\n\n# Plot\nplot(square_metres, price / 1000,\n     pch = 19, col = \"#1a1a2e90\",\n     xlab = \"Size (square metres)\",\n     ylab = \"Price (â‚¬ thousands)\",\n     main = \"House Prices: One Line Explains a Lot\",\n     family = \"sans\", cex.main = 1.2)\nabline(model$coefficients[1] / 1000, model$coefficients[2] / 1000,\n       col = \"#e94560\", lwd = 3)\n\n\n\n\n\n\n\n\nFigureÂ 1: A simple linear regression â€” the foundation beneath complex models\n\n\n\n\n\nThat red line? Thatâ€™s y = mx + b â€” something you learned in school. Itâ€™s also the building block of neural networks with millions of parameters. The only difference is scale."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#what-this-blog-will-cover",
    "href": "posts/01-why-stats-matter/index.html#what-this-blog-will-cover",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "What this blog will cover",
    "text": "What this blog will cover\nOver the coming weeks, weâ€™ll build your statistical intuition from the ground up. Each post will focus on one concept, explained so that:\n\nYou understand the why, not just the how\nYou can explain it to someone at a dinner party\nYou see how it connects to the AI/ML topics you care about\n\n\n\n\n\n\n\nTipKey Takeaway\n\n\n\nMachine learning is statistics at scale. Understanding the foundations doesnâ€™t slow you down â€” itâ€™s what separates someone who uses tools from someone who builds with them."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#coming-next-week",
    "href": "posts/01-why-stats-matter/index.html#coming-next-week",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "Coming next week",
    "text": "Coming next week\nWhat Data Actually Is â€” types, distributions, and why the shape of your data matters more than the algorithm you choose.\n\nFound this useful? Share it with someone whoâ€™s jumping into ML without the foundations. And follow along â€” we publish every week."
  }
]