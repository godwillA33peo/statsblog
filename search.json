[
  {
    "objectID": "posts/02-random-variables-distributions/index.html",
    "href": "posts/02-random-variables-distributions/index.html",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "",
    "text": "Open any ML tutorial. Step one is always the same: load the data. pd.read_csv('data.csv'). You get a DataFrame. Rows and columns. Numbers.\nAnd then you do things to those numbers. Scale them. Split them. Feed them to a model. Tune hyperparameters. Evaluate. Ship.\nBut here’s the question almost nobody stops to ask: where did those numbers come from?\nNot “which API” or “which database.” I mean: what process generated them? Is each row independent of the others? Could the values have turned out differently if you’d collected data on a different day? If you collected more data tomorrow, would the new rows look like the old ones?\nThese aren’t philosophical questions. They’re statistical ones. And your answers to them,whether you state them explicitly or not,determine whether your model’s predictions mean anything at all.\nThe language for thinking about this clearly is random variables and probability distributions. If you’ve seen these terms in a textbook and moved on, I’d like to show you why they’re not abstract theory,they’re the precise description of what your data is and what your model is trying to learn."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#your-data-has-a-secret-life",
    "href": "posts/02-random-variables-distributions/index.html#your-data-has-a-secret-life",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "",
    "text": "Open any ML tutorial. Step one is always the same: load the data. pd.read_csv('data.csv'). You get a DataFrame. Rows and columns. Numbers.\nAnd then you do things to those numbers. Scale them. Split them. Feed them to a model. Tune hyperparameters. Evaluate. Ship.\nBut here’s the question almost nobody stops to ask: where did those numbers come from?\nNot “which API” or “which database.” I mean: what process generated them? Is each row independent of the others? Could the values have turned out differently if you’d collected data on a different day? If you collected more data tomorrow, would the new rows look like the old ones?\nThese aren’t philosophical questions. They’re statistical ones. And your answers to them,whether you state them explicitly or not,determine whether your model’s predictions mean anything at all.\nThe language for thinking about this clearly is random variables and probability distributions. If you’ve seen these terms in a textbook and moved on, I’d like to show you why they’re not abstract theory,they’re the precise description of what your data is and what your model is trying to learn."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#a-random-variable-is-a-question-not-an-answer",
    "href": "posts/02-random-variables-distributions/index.html#a-random-variable-is-a-question-not-an-answer",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "A random variable is a question, not an answer",
    "text": "A random variable is a question, not an answer\nHere’s the definition you’ll find everywhere: a random variable is a function that maps outcomes from a sample space to real numbers.\nThat’s technically correct and practically useless. Let me give you a better one.\nA random variable is a numerical quantity whose value hasn’t been determined yet.\nBefore you observe your data, you don’t know what values you’ll get. Will this patient’s blood pressure be 120 or 145? Will this customer churn or stay? Will this image contain a cat? You don’t know. But you know the kind of values that are possible, and you may have beliefs about which values are more likely.\nThat’s what a random variable captures: the space of possible values and their relative likelihoods, before you observe anything.\nOnce you observe a value,once the patient’s blood pressure reads 132,that’s no longer a random variable. It’s a realisation. A data point. One draw from the underlying random process.\nYour entire dataset is a collection of realisations. Each row in your DataFrame was, before you observed it, a random variable. Now it’s a fixed number. But the process that generated it is still out there, and it could generate different numbers tomorrow.\nThis is why it matters: your model isn’t trying to memorise your specific 10,000 rows. It’s trying to learn the process that generated them, so it can make predictions about rows it hasn’t seen yet. You can’t do that without thinking about your data as draws from something larger.\n\nThe notation\nStatisticians use uppercase letters for random variables and lowercase for their observed values:\n\n\\(X\\) = the random variable (the question: “what will this patient’s blood pressure be?”)\n\\(x\\) = an observed value, a realisation (the answer: 132)\n\nWhen we write \\(X = x\\), we mean “the random variable \\(X\\) took the value \\(x\\).” When we write \\(P(X = x)\\), we mean “the probability that \\(X\\) takes the value \\(x\\).”\nThis isn’t pedantry. The distinction between \\(X\\) and \\(x\\) is the distinction between the process and the data. Confusing them is how you end up overfitting."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#distributions-the-shape-of-uncertainty",
    "href": "posts/02-random-variables-distributions/index.html#distributions-the-shape-of-uncertainty",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "Distributions: the shape of uncertainty",
    "text": "Distributions: the shape of uncertainty\nIf a random variable describes what could happen, a probability distribution describes how likely each possibility is.\nThink of it as a contract. Before you observe any data, the distribution tells you: “Here are all the possible values, and here’s the relative chance of each one.”\nThere are two flavours, depending on whether the random variable takes countable values or values on a continuum.\n\nDiscrete distributions: counting outcomes\nA discrete random variable takes values you can list: 0, 1, 2, 3, … or {cat, dog, bird}. The probability of each value is given by a probability mass function (PMF):\n\\[P(X = x) = p(x)\\]\nwith two rules: every probability is between 0 and 1, and they all add up to 1.\nThe Bernoulli distribution is the simplest possible distribution. A single trial. Two outcomes. Probability \\(p\\) of success, \\(1-p\\) of failure.\n\\[X \\sim \\text{Bernoulli}(p), \\quad P(X=1) = p, \\quad P(X=0) = 1-p\\]\nEvery binary classification target in your dataset follows a Bernoulli distribution. When your logistic regression outputs \\(\\hat{p} = 0.73\\), it’s estimating the parameter of a Bernoulli distribution. That’s literally what it’s doing,fitting \\(p\\).\nThe Binomial distribution counts the number of successes in \\(n\\) independent Bernoulli trials:\n\\[X \\sim \\text{Binomial}(n, p), \\quad P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\nIf you flip a coin 100 times, the number of heads is Binomial. If you classify 100 samples and count the correct ones, the number correct is Binomial (under independence). This is why your model’s accuracy has a sampling distribution,it’s not a fixed number, it’s a draw from a Binomial.\nThe Poisson distribution models the count of events in a fixed interval when events happen at a constant average rate:\n\\[X \\sim \\text{Poisson}(\\lambda), \\quad P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\]\nServer requests per minute. Customer complaints per day. Gene mutations per chromosome. Any time you’re modelling “how many times does something happen,” you’re probably looking at Poisson data.\n\n\nThese distributions are a family\nYou might have noticed something. The Bernoulli is a single yes/no trial. The Binomial counts how many yes’s you get across \\(n\\) independent Bernoulli trials. The Poisson emerges when \\(n\\) gets very large and \\(p\\) gets very small, but the expected count \\(np = \\lambda\\) stays fixed, it’s the limiting case of the Binomial for rare events. These aren’t three unrelated distributions. They’re connected, each one building on the last.\nIt goes deeper than that. The Normal distribution also connects to the Binomial, as \\(n\\) grows, the Binomial converges to a Normal (that’s the Central Limit Theorem at work). And all of these distributions, Bernoulli, Binomial, Poisson, Normal, and several others, belong to a single mathematical family called the exponential family, which turns out to be the foundation of generalised linear models (GLMs). We’ll explore these connections properly in a future article. For now, just know that when you learn one distribution, you’re not learning an isolated fact, you’re learning a node in a network.\n\n\nContinuous distributions: measuring on a continuum\nA continuous random variable takes values on the real line (or an interval). You can’t list all possible values, so you can’t assign a probability to any single value,\\(P(X = 1.23456789...) = 0\\) for any specific number.\nInstead, you describe probabilities over intervals using a probability density function (PDF):\n\\[P(a \\leq X \\leq b) = \\int_a^b f(x)\\,dx\\]\nThe PDF \\(f(x)\\) tells you how densely the probability is packed around each value. It’s not a probability itself,it can be greater than 1,but the area under the curve over any interval gives you a probability.\nThe Normal (Gaussian) distribution is the one you know:\n\\[X \\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\]\nTwo parameters: \\(\\mu\\) (the centre) and \\(\\sigma^2\\) (the spread). It shows up everywhere, and there’s a deep reason for that,the Central Limit Theorem, which we’ll cover in a future article.\nBut here’s what matters for ML: when you assume your regression errors are normally distributed, you’re saying the residuals follow this specific shape. That’s not a trivial assumption. It determines your loss function (MSE corresponds to Gaussian errors), your confidence intervals, and your hypothesis tests. If the assumption is wrong, all of those break.\nThe Uniform distribution assigns equal probability to all values in an interval:\n\\[X \\sim \\text{Uniform}(a, b), \\quad f(x) = \\frac{1}{b-a} \\quad \\text{for } a \\leq x \\leq b\\]\nThis is the distribution of “I have no idea what value to expect.” When you initialise neural network weights uniformly, you’re sampling from this distribution. When you use random search for hyperparameter tuning, you’re drawing from it. It’s the mathematical formalisation of ignorance."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#the-cdf-the-cumulative-view",
    "href": "posts/02-random-variables-distributions/index.html#the-cdf-the-cumulative-view",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "The CDF: the cumulative view",
    "text": "The CDF: the cumulative view\nThe cumulative distribution function (CDF) gives the probability that \\(X\\) is less than or equal to a value:\n\\[F(x) = P(X \\leq x)\\]\nFor discrete random variables, the CDF is a step function. For continuous ones, it’s a smooth curve from 0 to 1. The CDF always exists, even when the PDF doesn’t (for discrete variables), which makes it the more fundamental object.\nWhy should you care? Because when you compute a percentile, a quantile, or a p-value, you’re using the CDF. When you say “this patient’s blood pressure is in the 95th percentile,” you’re saying \\(F(x) = 0.95\\)."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#the-connection-to-ml-you-probably-missed",
    "href": "posts/02-random-variables-distributions/index.html#the-connection-to-ml-you-probably-missed",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "The connection to ML you probably missed",
    "text": "The connection to ML you probably missed\nHere’s where everything clicks.\nEvery supervised learning problem is an attempt to learn a conditional distribution.\nWhen you fit a regression model, you’re estimating:\n\\[P(Y \\mid X) = \\text{some distribution parameterised by } X\\]\nFor linear regression with Gaussian errors, this is:\n\\[Y \\mid X \\sim \\mathcal{N}(X\\beta, \\sigma^2)\\]\nFor logistic regression:\n\\[Y \\mid X \\sim \\text{Bernoulli}(\\sigma(X\\beta))\\]\nFor Poisson regression:\n\\[Y \\mid X \\sim \\text{Poisson}(\\exp(X\\beta))\\]\nIn each case, \\(X\\) (the features) determine the parameters of the distribution, and \\(Y\\) (the target) is a random draw from that distribution. The model doesn’t predict \\(Y\\) directly,it predicts the distribution of \\(Y\\), and the point prediction is just a summary (usually the mean).\nThis is why understanding distributions isn’t optional for ML. You’re fitting them whether you know it or not. sklearn just hides it from you.\n\nThe loss function is the distribution\nThis connection goes even deeper. Your choice of loss function implicitly assumes a distribution:\n\n\n\nLoss Function\nImplicit Distribution\nLink\n\n\n\n\nMean Squared Error\nNormal (Gaussian)\nIdentity\n\n\nCross-Entropy\nBernoulli / Categorical\nLogit / Softmax\n\n\nPoisson Deviance\nPoisson\nLog\n\n\nHuber Loss\nA compromise: Normal near 0, Laplace in the tails\n,\n\n\n\nWhen you minimise MSE, you’re doing maximum likelihood estimation under the assumption that your errors are Gaussian. When you minimise cross-entropy, you’re doing MLE under the assumption that your targets are Bernoulli.\nIf you’ve ever wondered “why MSE for regression and cross-entropy for classification?”,this is the answer. It’s not arbitrary. Each loss function is derived from a distributional assumption about the target variable. We’ll make this precise when we cover likelihood in a future article."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#what-this-means-in-practice",
    "href": "posts/02-random-variables-distributions/index.html#what-this-means-in-practice",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "What this means in practice",
    "text": "What this means in practice\nUnderstanding random variables and distributions gives you three practical superpowers:\n1. You can diagnose model failures. If your regression residuals aren’t approximately normal, MSE might not be the right loss. If your count data has more zeros than Poisson allows, you need a zero-inflated model. If your binary classifier’s predicted probabilities don’t match observed frequencies, it’s miscalibrated. All of these diagnoses require understanding the distributional assumptions you’ve made.\n2. You can quantify uncertainty. A prediction of \\(\\hat{y} = 42\\) is useless without knowing how confident you are. If you know the distribution, you can compute prediction intervals, confidence intervals, and posterior distributions. Without it, you’re flying blind.\n3. You can choose the right model. Gaussian targets → linear regression (or ridge, lasso). Binary targets → logistic regression. Count targets → Poisson regression. Skewed positive targets → Gamma regression. Bounded proportions → Beta regression. The distribution of your target variable tells you which model family to use."
  },
  {
    "objectID": "posts/02-random-variables-distributions/index.html#the-mental-model-to-take-away",
    "href": "posts/02-random-variables-distributions/index.html#the-mental-model-to-take-away",
    "title": "Random Variables and Distributions: What Your Data Actually Is",
    "section": "The mental model to take away",
    "text": "The mental model to take away\nHere’s the shift I want you to make:\nBefore: “I have data in a CSV. I’ll fit a model to it.”\nAfter: “My data is a sample of realisations from an unknown data-generating process. My model is a hypothesis about what that process looks like. Training is estimating the parameters of that process. Evaluation is checking whether my hypothesis is consistent with new realisations.”\nThat second framing is what statistics gives you. It’s more precise, more honest, and it leads to better models.\nNext week, we’ll build on this foundation. You now know that your data comes from distributions and your models estimate distributions. But what exactly are you estimating when you compute a mean, a variance, or a loss? That’s the world of expected values,and it turns out your loss function is one.\n\nThis is article 2 of Stats Beneath, a weekly series on the statistical foundations of machine learning. Subscribe to get each article when it’s published."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The statistical why behind every ML algorithm",
    "section": "",
    "text": "For ML engineers and data scientists who want to understand the mathematics driving their models, not just tune the hyperparameters. Intuition first, code second.\nNew article every week.\n\nBrowse Articles Subscribe"
  },
  {
    "objectID": "index.html#latest-articles",
    "href": "index.html#latest-articles",
    "title": "The statistical why behind every ML algorithm",
    "section": "Latest Articles",
    "text": "Latest Articles\n\n\n\n\n\nRandom Variables and Distributions: What Your Data Actually Is\n\n\n\nfoundations\n\nprobability\n\n\n\nYour dataset isn’t just numbers in a CSV. It’s a realisation of random variables drawn from an unknown distribution. Until you understand that, every model you build is a guess about a process you haven’t described.\n\n\n\n\n\nFeb 13, 2026\n\n\n\n\n\n\n\nWhy Statistics Matters Before You Touch Machine Learning\n\n\n\nfoundations\n\nmachine-learning\n\n\n\nEveryone wants to build AI. But the real superpower isn’t knowing which algorithm to use,it’s understanding why it works.\n\n\n\n\n\nFeb 10, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "G\n\n\n\n\nMSc Health Data Science · Bayesian Modelling · Statistical Foundations for ML"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "About me",
    "text": "About me\nI’m a Health Data Science MSc student at the University of Galway, originally from Zimbabwe. I hold a First Class Honours degree in Operations Research and Statistics, and have spent years working as a Monitoring & Evaluation Specialist in the health sector, where I developed a deep appreciation for rigorous data analysis and evidence-based decision making.\nIn 2023, I was selected for the Ireland Fellows Programme, which brought me to Ireland to pursue advanced studies in health data science. My journey through operations research, public health program evaluation, and now Bayesian statistical modelling has taught me that the real power of data science lies not in knowing which algorithm to run, but in understanding why it works.\nI created Stats Beneath because I’ve seen too many talented engineers and data scientists skip over the statistical foundations that make machine learning actually work. This blog is my attempt to bridge that gap, explaining one statistical concept at a time, with clarity, intuition, and real-world context."
  },
  {
    "objectID": "about.html#what-im-working-on",
    "href": "about.html#what-im-working-on",
    "title": "About",
    "section": "What I’m working on",
    "text": "What I’m working on\nMSc Thesis: I’m currently working on small area estimation for malaria prevalence in Ghana using Bayesian hierarchical models. Specifically, I’m implementing BYM2 and ICAR spatial models in Stan to produce district-level prevalence estimates from survey data. This work combines my interests in public health, spatial statistics, and Bayesian inference.\nStats Beneath: This blog, where I publish weekly deep dives into the statistical concepts that power machine learning and AI. Each article starts with intuition, builds to mathematical understanding, and ends with practical takeaways.\nBayesian Explorer: An interactive Shiny application I built to help students and practitioners develop intuition for Bayesian concepts. It visualizes prior-likelihood-posterior relationships and lets you explore how different priors affect inference in real-time. Try it here →"
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About",
    "section": "Technical skills",
    "text": "Technical skills\nI work primarily in R for statistical analysis and modeling, using Stan and JAGS for Bayesian inference. I build interactive tools with Shiny, write and publish technical content with Quarto, and handle version control with Git. I also use Python for machine learning workflows and have experience with geospatial analysis (spatial statistics, mapping, GIS workflows).\nMy approach is tool-agnostic but principles-first: I believe understanding the statistical foundations makes you better at choosing the right tool for the job."
  },
  {
    "objectID": "about.html#connect",
    "href": "about.html#connect",
    "title": "About",
    "section": "Connect",
    "text": "Connect\nI’m always interested in connecting with people working at the intersection of statistics, machine learning, and public health.\n\n\nGitHub, code, projects, and open-source contributions\nLinkedIn, professional background and updates\nEmail, reach out directly\n\n\n\nThis site is built with Quarto and deployed on GitHub Pages. All content is written in R Markdown and rendered to HTML. The source code is available on GitHub."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "All Articles",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nCategories\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nFebruary 13, 2026\n\n\nRandom Variables and Distributions: What Your Data Actually Is\n\n\nfoundations, probability\n\n\n10 min\n\n\n\n\n\n\nFebruary 10, 2026\n\n\nWhy Statistics Matters Before You Touch Machine Learning\n\n\nfoundations, machine-learning\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html",
    "href": "posts/01-why-stats-matter/index.html",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "",
    "text": "Did you know?\n\nThe term “regression” comes from Francis Galton’s 1886 study on the heights of parents and children. He noticed that tall parents tended to have slightly shorter children, and short parents had slightly taller ones,they “regressed” toward the average. The name stuck, even though modern regression has nothing to do with shrinking.\n\nThere’s a pattern I see everywhere. Someone discovers machine learning, gets excited, installs scikit-learn or tidymodels, runs a random forest on a dataset, and gets 94% accuracy. Amazing! Ship it!\nBut then someone asks: “Why did your model predict that?” And the answer is… silence.\nThis is what happens when we skip the foundation. Machine learning is not magic,it’s statistics at scale. And if you don’t understand the statistics beneath the algorithm, you’re building on sand."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#the-rush-to-the-algorithm",
    "href": "posts/01-why-stats-matter/index.html#the-rush-to-the-algorithm",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "",
    "text": "Did you know?\n\nThe term “regression” comes from Francis Galton’s 1886 study on the heights of parents and children. He noticed that tall parents tended to have slightly shorter children, and short parents had slightly taller ones,they “regressed” toward the average. The name stuck, even though modern regression has nothing to do with shrinking.\n\nThere’s a pattern I see everywhere. Someone discovers machine learning, gets excited, installs scikit-learn or tidymodels, runs a random forest on a dataset, and gets 94% accuracy. Amazing! Ship it!\nBut then someone asks: “Why did your model predict that?” And the answer is… silence.\nThis is what happens when we skip the foundation. Machine learning is not magic,it’s statistics at scale. And if you don’t understand the statistics beneath the algorithm, you’re building on sand."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#what-beneath-actually-means",
    "href": "posts/01-why-stats-matter/index.html#what-beneath-actually-means",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "What “beneath” actually means",
    "text": "What “beneath” actually means\nEvery machine learning model you’ve ever heard of is built on statistical concepts that have existed for decades:\nLinear regression (1800s) → Neural networks are layers of linear regressions with activation functions stacked on top.\nBayes’ theorem (1763) → The entire field of Bayesian machine learning, spam filters, and medical diagnosis models.\nMatrix multiplication (1850s) → Literally how your data flows through every neural network.\nProbability distributions (1700s) → How models quantify uncertainty, make predictions, and learn from data.\nThe “AI revolution” isn’t new math. It’s old math with new computers."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#a-simple-example",
    "href": "posts/01-why-stats-matter/index.html#a-simple-example",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "A simple example",
    "text": "A simple example\nLet’s say you want to predict house prices. A machine learning tutorial might tell you to throw your data into XGBoost. But what’s XGBoost actually doing?\nAt its core, it’s fitting a sequence of simple models (like decision trees) where each new model tries to fix the mistakes of the previous ones. The “mistakes” are measured using a loss function,which is a statistical concept. The way it finds the best split in a tree? Variance reduction,another statistical concept.\nLet’s see this with the simplest possible model,a straight line:\n\n\nCode\n# Generate some example data\nset.seed(42)\nn &lt;- 50\nsquare_metres &lt;- runif(n, 40, 200)\nprice &lt;- 50000 + 2500 * square_metres + rnorm(n, 0, 25000)\n\n# Fit the simplest model\nmodel &lt;- lm(price ~ square_metres)\n\n# Plot\nplot(square_metres, price / 1000,\n     pch = 19, col = \"#1a1a2e90\",\n     xlab = \"Size (square metres)\",\n     ylab = \"Price (€ thousands)\",\n     main = \"House Prices: One Line Explains a Lot\",\n     family = \"sans\", cex.main = 1.2)\nabline(model$coefficients[1] / 1000, model$coefficients[2] / 1000,\n       col = \"#e94560\", lwd = 3)\n\n\n\n\n\n\n\n\nFigure 1: A simple linear regression,the foundation beneath complex models\n\n\n\n\n\nThat red line? That’s y = mx + b,something you learned in school. It’s also the building block of neural networks with millions of parameters. The only difference is scale."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#what-this-blog-will-cover",
    "href": "posts/01-why-stats-matter/index.html#what-this-blog-will-cover",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "What this blog will cover",
    "text": "What this blog will cover\nOver the coming weeks, we’ll build your statistical intuition from the ground up. Each post will focus on one concept, explained so that:\n\nYou understand the why, not just the how\nYou can explain it to someone at a dinner party\nYou see how it connects to the AI/ML topics you care about\n\n\n\n\n\n\n\nTipKey Takeaway\n\n\n\nMachine learning is statistics at scale. Understanding the foundations doesn’t slow you down,it’s what separates someone who uses tools from someone who builds with them."
  },
  {
    "objectID": "posts/01-why-stats-matter/index.html#coming-next-week",
    "href": "posts/01-why-stats-matter/index.html#coming-next-week",
    "title": "Why Statistics Matters Before You Touch Machine Learning",
    "section": "Coming next week",
    "text": "Coming next week\nWhat Data Actually Is,types, distributions, and why the shape of your data matters more than the algorithm you choose.\n\nFound this useful? Share it with someone who’s jumping into ML without the foundations. And follow along,we publish every week."
  }
]